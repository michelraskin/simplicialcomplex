{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchmetrics.classification import MulticlassAUROC, MulticlassAccuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split,GroupShuffleSplit\n",
    "from captum.attr import IntegratedGradients, Occlusion\n",
    "import warnings\n",
    "import re\n",
    "from glob import glob\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple GPU\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "if os.path.isfile(\"myData.npz\"):\n",
    "    print(\"✅ Loading preprocessed features...\")\n",
    "    with np.load(\"myData.npz\") as data:\n",
    "        myData = data['myData']     \n",
    "        myData2 = data['myData2']  \n",
    "        myY = data['myY']       \n",
    "        myActors = data['myActors']   \n",
    "        myDatasets = data['myDatasets'] \n",
    "\n",
    "    print(f\"Mel spectrograms shape: {myData.shape}\")\n",
    "    print(f\"Topological features shape: {myData2.shape}\")\n",
    "    print(f\"Labels shape: {myY.shape}\")\n",
    "    print(f\"Number of unique actors: {len(np.unique(myActors))}\")\n",
    "    print(f\"Number of unique datasets: {len(np.unique(myDatasets))}\")\n",
    "else:\n",
    "    folder = './savefiles3'\n",
    "\n",
    "    def findFilesFromPattern(pattern):\n",
    "        pattern = re.compile(pattern + r'_(.*?)_(.*?)_(.*?)_(\\d+)_(\\d+)\\.npy')\n",
    "        heatmaps_dict = {}\n",
    "\n",
    "        for filename in os.listdir(folder):\n",
    "            match = pattern.match(filename)\n",
    "            if match:\n",
    "                dataset, actor, emotion, i, j = map(str, match.groups())\n",
    "                i, j = int(i), int(j)\n",
    "                filepath = os.path.join(folder, filename)\n",
    "                data = np.load(filepath)\n",
    "\n",
    "                heatmaps_dict[f'{dataset}_{actor}_{emotion}_{j // 2}_{j%2}'] = {'data': data, 'dataset': dataset, 'actor': actor, 'emotion':emotion, 'type': j}\n",
    "\n",
    "        return heatmaps_dict\n",
    "\n",
    "    melwasserstein = findFilesFromPattern('wassersteinHeat')\n",
    "    meltimeeuclid = findFilesFromPattern('timeMetricHeat')\n",
    "    meleuclid = findFilesFromPattern('euclideanHeat')\n",
    "\n",
    "    def load_spectrograms(prefixes, path='./savefiles3'):\n",
    "        patterns = []\n",
    "        for prefix in prefixes:\n",
    "            patterns.append(os.path.join(path, f\"{prefix}_*.npy\"))\n",
    "        my_globs = glob(patterns[0])\n",
    "        for pattern in patterns[1:]:\n",
    "            my_globs = my_globs + glob(pattern)\n",
    "        file_list = sorted(my_globs)\n",
    "        return [np.load(file) for i, file in enumerate(file_list)]\n",
    "\n",
    "    myRaw = load_spectrograms([\"savee\", 'tess', 'radvess', 'cremad'])\n",
    "    print(len(melwasserstein))\n",
    "    print(len([melwasserstein[key]['data'] for key in sorted(melwasserstein.keys()) if melwasserstein[key]['type'] % 2 == 0]))\n",
    "    print(len([melwasserstein[key]['data'] for key in sorted(melwasserstein.keys()) if melwasserstein[key]['type'] % 2 == 1]))\n",
    "    print(np.array([[meleuclid[key]['data'] for key in sorted(meleuclid.keys()) if meleuclid[key]['type'] == 0]]).shape)\n",
    "\n",
    "    print(len(myRaw))\n",
    "\n",
    "    myData = np.array([myRaw])\n",
    "    print('finish data')\n",
    "    myData = myData.astype('float32')\n",
    "    myData = np.transpose(myData, (1, 2, 3, 0))\n",
    "    myEmotionMap = {\n",
    "        'neutral': 1, 'happy':2, 'sad':3, 'angry':4, 'fearful':5, 'disgust':6, 'calm':7, 'surprised':8\n",
    "    }\n",
    "    myY = np.array(\n",
    "        [myEmotionMap[melwasserstein[key]['emotion']] -1 for key in sorted(melwasserstein.keys()) if melwasserstein[key]['type'] % 2 == 0]\n",
    "    )\n",
    "    myActors = np.array(\n",
    "        [melwasserstein[key]['actor'] + '_' + melwasserstein[key]['dataset']  for key in sorted(melwasserstein.keys()) if melwasserstein[key]['type'] % 2 == 0]\n",
    "    )\n",
    "    myDatasets = np.array(\n",
    "        [melwasserstein[key]['dataset']  for key in sorted(melwasserstein.keys()) if melwasserstein[key]['type'] % 2 == 0]\n",
    "    )\n",
    "    print(np.unique(myActors))\n",
    "\n",
    "    print(np.unique(myY))\n",
    "\n",
    "    myY = to_categorical(myY, num_classes=6)\n",
    "\n",
    "    myData2 = np.array([\n",
    "                        [meleuclid[key]['data'] for key in sorted(meleuclid.keys()) if meleuclid[key]['type'] % 2 == 0],\n",
    "                        [meleuclid[key]['data'] for key in sorted(meleuclid.keys()) if meleuclid[key]['type'] % 2 == 1],\n",
    "                        [meltimeeuclid[key]['data'] for key in sorted(meltimeeuclid.keys()) if meltimeeuclid[key]['type'] % 2 == 0],\n",
    "                        [meltimeeuclid[key]['data'] for key in sorted(meltimeeuclid.keys()) if meltimeeuclid[key]['type'] % 2 == 1],\n",
    "                        [melwasserstein[key]['data'] for key in sorted(melwasserstein.keys()) if melwasserstein[key]['type'] % 2 == 0],\n",
    "                        [melwasserstein[key]['data'] for key in sorted(melwasserstein.keys()) if melwasserstein[key]['type'] % 2 == 1]\n",
    "                        ])\n",
    "    print('finish data')\n",
    "    myData2 = myData2.astype('float32')\n",
    "    print(myData2.shape)\n",
    "    myData2 = np.transpose(myData2, (1, 2, 3, 0))\n",
    "    print(myData2.shape)\n",
    "    np.savez_compressed(\n",
    "        \"myData.npz\",\n",
    "        myData=myData,\n",
    "        myData2=myData2,\n",
    "        myY=myY,\n",
    "        myActors=myActors,\n",
    "        myDatasets=myDatasets,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_topological_channels(data, labels, actors, num_samples=3):\n",
    "    emotions = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust']\n",
    "    channel_names = [\n",
    "        'Euclidean (H0)',\n",
    "        'Euclidean (H1)',\n",
    "        'Temporal (H0)',\n",
    "        'Temporal (H1)',\n",
    "        'Wasserstein (H0)',\n",
    "        'Wasserstein (H1)'\n",
    "    ]\n",
    "\n",
    "    label_ids = np.argmax(labels, axis=1)\n",
    "\n",
    "    for emotion_id in range(6):\n",
    "        emotion_mask = label_ids == emotion_id\n",
    "        emotion_samples = data[emotion_mask]\n",
    "\n",
    "        indices = np.random.choice(len(emotion_samples), min(num_samples, len(emotion_samples)), replace=False)\n",
    "\n",
    "        fig, axes = plt.subplots(num_samples, 6, figsize=(18, 3*num_samples))\n",
    "        fig.suptitle(f'Topological Features: {emotions[emotion_id].upper()}', fontsize=16, y=1.00)\n",
    "\n",
    "        for row, idx in enumerate(indices):\n",
    "            sample = emotion_samples[idx]  \n",
    "            for col in range(6):\n",
    "                ax = axes[row, col] if num_samples > 1 else axes[col]\n",
    "                ax.imshow(sample[:, :, col], cmap='viridis', aspect='auto')\n",
    "                if row == 0:\n",
    "                    ax.set_title(channel_names[col], fontsize=10)\n",
    "                ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'topology_{emotions[emotion_id]}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Saved: topology_{emotions[emotion_id]}.png\\n\")\n",
    "\n",
    "if 'myData2' in locals():\n",
    "    print(\"Generating topological feature visualizations...\")\n",
    "    visualize_topological_channels(myData2, myY, myActors, num_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelSpectrogramCNN(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(MelSpectrogramCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class TopologicalCNN(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(TopologicalCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(6, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        f = self.features(x)\n",
    "        z = self.classifier[:-1](f)  \n",
    "        logits = self.classifier[-1](z)\n",
    "        return z if return_embedding else logits\n",
    "\n",
    "\n",
    "class CombinedFusionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(CombinedFusionModel, self).__init__()\n",
    "        self.mel_branch = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.topo_branch = nn.Sequential(\n",
    "            nn.Conv2d(6, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, mel_input, topo_input):\n",
    "        mel_emb = self.mel_branch(mel_input)\n",
    "        topo_emb = self.topo_branch(topo_input)\n",
    "        combined = torch.cat([mel_emb, topo_emb], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "print(\"✅ Model architectures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def stratified_group_split_3way(\n",
    "    y,\n",
    "    groups,\n",
    "    myDatasets,\n",
    "    val_size=0.2,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "):\n",
    "    y = np.asarray(y)\n",
    "    groups = np.asarray(groups)\n",
    "    myDatasets = np.asarray(myDatasets)\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    df = pd.DataFrame({\"y\": y, \"group\": groups, \"dataset\": myDatasets})\n",
    "\n",
    "    group_df = (\n",
    "        df.groupby(\"group\")\n",
    "          .agg(\n",
    "              y_group=(\"y\", lambda s: s.value_counts().index[0]),\n",
    "              dataset_group=(\"dataset\", lambda s: s.value_counts().index[0]),\n",
    "          )\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    datasets = group_df[\"dataset_group\"].unique()\n",
    "    n_groups = len(group_df)\n",
    "\n",
    "    ds_counts = group_df[\"dataset_group\"].value_counts()\n",
    "    bad = ds_counts[ds_counts < 2].index.to_list()\n",
    "    if bad:\n",
    "        raise ValueError(f\"Cannot guarantee dataset coverage in both train and test; datasets with <2 groups: {bad}\")\n",
    "\n",
    "    # Target group counts\n",
    "    n_test = max(int(np.round(test_size * n_groups)), len(datasets))\n",
    "    n_val = int(np.round(val_size * n_groups))\n",
    "\n",
    "    if n_test + n_val >= n_groups:\n",
    "        raise ValueError(\"Not enough groups to allocate train/val/test with these sizes.\")\n",
    "\n",
    "    # After taking test+val, train must still have at least one per dataset\n",
    "    if (n_groups - n_test - n_val) < len(datasets):\n",
    "        raise ValueError(\n",
    "            \"Not enough remaining groups to guarantee at least one group per dataset in TRAIN. \"\n",
    "            \"Reduce val_size and/or test_size.\"\n",
    "        )\n",
    "\n",
    "    forced_test = []\n",
    "    forced_train = []\n",
    "\n",
    "    for ds in datasets:\n",
    "        ds_groups = group_df[group_df[\"dataset_group\"] == ds].copy()\n",
    "\n",
    "        y_counts = ds_groups[\"y_group\"].value_counts()\n",
    "        labels = y_counts.index.to_numpy()\n",
    "        probs = (y_counts / y_counts.sum()).to_numpy()\n",
    "        chosen_label = rng.choice(labels, p=probs)\n",
    "\n",
    "        candidates = ds_groups.loc[ds_groups[\"y_group\"] == chosen_label, \"group\"].to_numpy()\n",
    "        g_test = rng.choice(candidates)\n",
    "        forced_test.append(g_test)\n",
    "\n",
    "        # choose a different group for train\n",
    "        remaining = ds_groups.loc[ds_groups[\"group\"] != g_test, \"group\"].to_numpy()\n",
    "        g_train = rng.choice(remaining)\n",
    "        forced_train.append(g_train)\n",
    "\n",
    "    forced_test = np.array(pd.unique(forced_test))\n",
    "    forced_train = np.array(pd.unique(forced_train))\n",
    "\n",
    "    overlap = set(forced_train).intersection(set(forced_test))\n",
    "    if overlap:\n",
    "        for g in list(overlap):\n",
    "            ds = group_df.loc[group_df[\"group\"] == g, \"dataset_group\"].iloc[0]\n",
    "            ds_groups = group_df[group_df[\"dataset_group\"] == ds][\"group\"].to_numpy()\n",
    "            used = set(forced_test).union(set(forced_train))\n",
    "            repl = next((x for x in ds_groups if x not in used), None)\n",
    "            if repl is None:\n",
    "                repl = next(x for x in ds_groups if x not in set(forced_test))\n",
    "            forced_train[forced_train == g] = repl\n",
    "\n",
    "    forced_test_set = set(forced_test.tolist())\n",
    "    forced_train_set = set(forced_train.tolist())\n",
    "\n",
    "    # -------------------------\n",
    "    # Fill remaining TEST slots (IMPORTANT: never sample forced_train into test)\n",
    "    # -------------------------\n",
    "    remaining_test_slots = n_test - len(forced_test_set)\n",
    "    eligible_for_test = group_df[\n",
    "        (~group_df[\"group\"].isin(forced_test_set)) &\n",
    "        (~group_df[\"group\"].isin(forced_train_set))\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    extra_test = np.array([], dtype=group_df[\"group\"].dtype)\n",
    "    if remaining_test_slots > 0:\n",
    "        rem_n = len(eligible_for_test)\n",
    "        if remaining_test_slots > rem_n:\n",
    "            # If this happens, expand eligibility by allowing forced_train (but then train coverage may break).\n",
    "            # Better to fail loudly.\n",
    "            raise ValueError(\n",
    "                \"Not enough eligible groups to fill test without stealing reserved train-coverage groups. \"\n",
    "                \"Reduce test_size.\"\n",
    "            )\n",
    "\n",
    "        if remaining_test_slots == rem_n:\n",
    "            extra_test = eligible_for_test[\"group\"].to_numpy()\n",
    "        else:\n",
    "            sss = StratifiedShuffleSplit(\n",
    "                n_splits=1,\n",
    "                test_size=remaining_test_slots / rem_n,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "            idx = np.arange(rem_n)\n",
    "            _, extra_idx = next(sss.split(idx, eligible_for_test[\"y_group\"]))\n",
    "            extra_test = eligible_for_test.loc[extra_idx, \"group\"].to_numpy()\n",
    "\n",
    "    test_groups = np.array(list(forced_test_set.union(set(extra_test.tolist()))))\n",
    "    test_set = set(test_groups.tolist())\n",
    "\n",
    "    # -------------------------\n",
    "    # Build remaining pool for TRAIN/VAL\n",
    "    #   - Keep forced_train always in TRAIN (never in VAL)\n",
    "    # -------------------------\n",
    "    remaining_after_test = group_df[~group_df[\"group\"].isin(test_set)].reset_index(drop=True)\n",
    "\n",
    "    split_pool = remaining_after_test[~remaining_after_test[\"group\"].isin(forced_train_set)].reset_index(drop=True)\n",
    "\n",
    "    n_val = min(n_val, len(split_pool))\n",
    "    if n_val <= 0:\n",
    "        val_set = set()\n",
    "        train_set = set(remaining_after_test[\"group\"].to_list())  # includes forced_train\n",
    "    else:\n",
    "        sss_inner = StratifiedShuffleSplit(\n",
    "            n_splits=1,\n",
    "            test_size=n_val / len(split_pool),\n",
    "            random_state=random_state + 1,\n",
    "        )\n",
    "        idx = np.arange(len(split_pool))\n",
    "        train_idx_g, val_idx_g = next(sss_inner.split(idx, split_pool[\"y_group\"]))\n",
    "\n",
    "        val_set = set(split_pool.loc[val_idx_g, \"group\"].to_list())\n",
    "        train_set = set(remaining_after_test[\"group\"].to_list()) - val_set  # ensures disjoint, includes forced_train\n",
    "\n",
    "    # -------------------------\n",
    "    # Sanity checks\n",
    "    # -------------------------\n",
    "    if train_set & test_set:\n",
    "        raise RuntimeError(\"Train/Test overlap detected (groups).\")\n",
    "    if val_set & test_set:\n",
    "        raise RuntimeError(\"Val/Test overlap detected (groups).\")\n",
    "    if train_set & val_set:\n",
    "        raise RuntimeError(\"Train/Val overlap detected (groups).\")\n",
    "\n",
    "    expected_ds = set(group_df[\"dataset_group\"].unique())\n",
    "    got_test = set(group_df.loc[group_df[\"group\"].isin(list(test_set)), \"dataset_group\"].unique())\n",
    "    got_train = set(group_df.loc[group_df[\"group\"].isin(list(train_set)), \"dataset_group\"].unique())\n",
    "\n",
    "    if expected_ds - got_test:\n",
    "        raise RuntimeError(f\"Dataset coverage constraint failed in TEST: missing {expected_ds - got_test}\")\n",
    "    if expected_ds - got_train:\n",
    "        raise RuntimeError(f\"Dataset coverage constraint failed in TRAIN: missing {expected_ds - got_train}\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Map groups -> sample indices\n",
    "    # -------------------------\n",
    "    train_idx = np.where(df[\"group\"].isin(list(train_set)))[0]\n",
    "    val_idx = np.where(df[\"group\"].isin(list(val_set)))[0]\n",
    "    test_idx = np.where(df[\"group\"].isin(list(test_set)))[0]\n",
    "\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Perform split\n",
    "if 'myY' in locals():\n",
    "    groups = myActors\n",
    "    train_idx, val_idx, test_idx = stratified_group_split_3way(\n",
    "        y=np.argmax(myY, axis=1),\n",
    "        myDatasets=myDatasets,\n",
    "        groups=groups,\n",
    "        val_size=0.2,\n",
    "        test_size=0.2\n",
    "    )\n",
    "\n",
    "    print(f\"Train speakers: {len(np.unique(myActors[train_idx]))}\")\n",
    "    print(f\"Train datasets: {len(np.unique(myDatasets[train_idx]))}\")\n",
    "    print(f\"Val speakers: {len(np.unique(myActors[val_idx]))}\")\n",
    "    print(f\"Test speakers: {len(np.unique(myActors[test_idx]))}\")\n",
    "    print(f\"Test datasets: {len(np.unique(myDatasets[test_idx]))}\")\n",
    "\n",
    "    print(f\"\\nTrain samples: {len(train_idx)}\")\n",
    "    print(f\"Val samples: {len(val_idx)}\")\n",
    "    print(f\"Test samples: {len(test_idx)}\")\n",
    "\n",
    "    # Global split\n",
    "    X_train, X_val, X_test = myData[train_idx], myData[val_idx], myData[test_idx]\n",
    "    X_train2, X_val2, X_test2 = myData2[train_idx], myData2[val_idx], myData2[test_idx]\n",
    "    y_train, y_val, y_test = myY[train_idx], myY[val_idx], myY[test_idx]\n",
    "\n",
    "    # myRavdessMask = [x == 'radvess' for x in myDatasets]\n",
    "    # myNoRavdessMask = [x != 'radvess' for x in myDatasets]\n",
    "    # sgss = GroupShuffleSplit(\n",
    "    #     n_splits=1,\n",
    "    #     test_size=0.2,\n",
    "    #     random_state=42\n",
    "    # )\n",
    "    # test_idx = myRavdessMask\n",
    "    # train_idx, val_idx = next(sgss.split(myData[myNoRavdessMask], groups=groups[myNoRavdessMask]))\n",
    "    # X_train, X_val, X_test = myData[myNoRavdessMask][train_idx], myData[myNoRavdessMask][val_idx], myData[myRavdessMask]\n",
    "    # X_train2, X_val2, X_test2 = myData2[myNoRavdessMask][train_idx], myData2[myNoRavdessMask][val_idx], myData2[myRavdessMask]\n",
    "    # y_train, y_val, y_test = myY[myNoRavdessMask][train_idx], myY[myNoRavdessMask][val_idx], myY[myRavdessMask]\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train.transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "    X_train2_tensor = torch.tensor(X_train2.transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(np.argmax(y_train, axis=1), dtype=torch.long)\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val.transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "    X_val2_tensor = torch.tensor(X_val2.transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(np.argmax(y_val, axis=1), dtype=torch.long)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test.transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "    X_test2_tensor = torch.tensor(X_test2.transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(np.argmax(y_test, axis=1), dtype=torch.long)\n",
    "\n",
    "    print(f\"\\nTensor shapes:\")\n",
    "    print(f\"Train mel: {X_train_tensor.shape}\")\n",
    "    print(f\"Train topo: {X_train2_tensor.shape}\")\n",
    "    print(f\"Test mel: {X_test_tensor.shape}\")\n",
    "    print(f\"Test topo: {X_test2_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myData[myNoRavdessMask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(groups[myNoRavdessMask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myTessMask = [dataset != 'r' for dataset in myDatasets[train_idx]]\n",
    "for i in (np.unique(myActors[myNoRavdessMask][train_idx])):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (np.unique(myActors[test_idx])):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (np.unique(myActors[val_idx])):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=30, lr=1e-3,\n",
    "                model_name=\"model\", device=device):\n",
    "    \"\"\"\n",
    "    Generic training function with validation.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    auroc = MulticlassAUROC(num_classes=6, average='macro').to(device)\n",
    "    top3acc = MulticlassAccuracy(num_classes=6, top_k=3).to(device)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_auc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            if len(batch) == 3:  # Combined model\n",
    "                X_batch, X2_batch, y_batch = batch\n",
    "                X_batch, X2_batch, y_batch = X_batch.to(device), X2_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch, X2_batch)\n",
    "            else: \n",
    "                X_batch, y_batch = batch\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                if len(batch) == 3:\n",
    "                    X_val, X2_val, y_val = batch\n",
    "                    X_val, X2_val, y_val = X_val.to(device), X2_val.to(device), y_val.to(device)\n",
    "                    outputs = model(X_val, X2_val)\n",
    "                else:\n",
    "                    X_val, y_val = batch\n",
    "                    X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                    outputs = model(X_val)\n",
    "\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.softmax(outputs, dim=1)\n",
    "                val_preds.append(preds)\n",
    "                val_labels.append(y_val)\n",
    "\n",
    "        val_preds = torch.cat(val_preds)\n",
    "        val_labels = torch.cat(val_labels)\n",
    "\n",
    "        val_auc = auroc(val_preds, val_labels).item()\n",
    "        val_top3 = top3acc(val_preds, val_labels).item()\n",
    "        y_pred = torch.argmax(val_preds, dim=1)\n",
    "        accuracy = (y_pred == val_labels).float().mean().item()\n",
    "\n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "        history['val_loss'].append(val_loss / len(val_loader))\n",
    "        history['val_auc'].append(val_auc)\n",
    "        history['val_acc'].append(accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"train_loss: {train_loss/len(train_loader):.4f} - \"\n",
    "              f\"val_loss: {val_loss/len(val_loader):.4f} - \"\n",
    "              f\"val_auc: {val_auc:.4f} - \"\n",
    "              f\"val_top3: {val_top3:.4f} - \"\n",
    "              f\"val_acc: {accuracy:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            torch.save(model.state_dict(), f\"best_{model_name}.pth\")\n",
    "            print(f\"✅ Saved best model: {accuracy:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def evaluate_model(model, test_loader, model_path, device=device):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set and compute accuracy per dataset.\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    auroc = MulticlassAUROC(num_classes=6, average='macro').to(device)\n",
    "    top3acc = MulticlassAccuracy(num_classes=6, top_k=3).to(device)\n",
    "\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "\n",
    "    # Per-dataset tracking\n",
    "    dataset_correct = defaultdict(int)\n",
    "    dataset_total = defaultdict(int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "\n",
    "            if len(batch) == 4:\n",
    "                # Two-input model\n",
    "                X_batch, X2_batch, y_batch, dataset_batch = batch\n",
    "                X_batch = X_batch.to(device)\n",
    "                X2_batch = X2_batch.to(device)\n",
    "                outputs = model(X_batch, X2_batch)\n",
    "\n",
    "            elif len(batch) == 3:\n",
    "                # Single-input model\n",
    "                X_batch, y_batch, dataset_batch = batch\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected batch format\")\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            # Store global results\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "            all_probs.append(probs)\n",
    "\n",
    "            # Per-dataset accuracy\n",
    "            for pred, label, ds in zip(preds.cpu(), y_batch, dataset_batch):\n",
    "                dataset_total[ds] += 1\n",
    "                if pred == label:\n",
    "                    dataset_correct[ds] += 1\n",
    "\n",
    "    all_probs = torch.cat(all_probs)\n",
    "    all_labels_tensor = torch.tensor(all_labels).to(device)\n",
    "\n",
    "    test_auc = auroc(all_probs, all_labels_tensor).item()\n",
    "    test_top3 = top3acc(all_probs, all_labels_tensor).item()\n",
    "    test_acc = (torch.argmax(all_probs, dim=1) == all_labels_tensor).float().mean().item()\n",
    "\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"AUC: {test_auc:.4f}\")\n",
    "    print(f\"Top-3 Accuracy: {test_top3:.4f}\")\n",
    "    print(f\"Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # Per-dataset accuracy\n",
    "    print(\"\\nAccuracy per dataset:\")\n",
    "    dataset_acc = {}\n",
    "    for ds in dataset_total:\n",
    "        acc = dataset_correct[ds] / dataset_total[ds]\n",
    "        dataset_acc[ds] = acc\n",
    "        print(f\"{ds}: {acc:.4f}\")\n",
    "\n",
    "    # Classification report\n",
    "    class_labels = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust']\n",
    "    report = classification_report(all_labels, all_preds, target_names=class_labels, output_dict=True)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm_df = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        'auc': test_auc,\n",
    "        'top3_acc': test_top3,\n",
    "        'acc': test_acc,\n",
    "        'acc_per_dataset': dataset_acc,\n",
    "        'predictions': all_preds,\n",
    "        'labels': all_labels,\n",
    "        'report': report\n",
    "    }\n",
    "\n",
    "def get_macro_f1(res):\n",
    "    # classification_report output dict typically has:\n",
    "    # res[\"report\"][\"macro avg\"][\"f1-score\"]\n",
    "    return float(res[\"report\"][\"macro avg\"][\"f1-score\"])\n",
    "\n",
    "def summarize_with_f1(all_results):\n",
    "    aucs  = np.array([r[\"auc\"] for r in all_results], dtype=float)\n",
    "    accs  = np.array([r[\"acc\"] for r in all_results], dtype=float)\n",
    "    top3s = np.array([r[\"top3_acc\"] for r in all_results], dtype=float)\n",
    "    f1s   = np.array([get_macro_f1(r) for r in all_results], dtype=float)\n",
    "\n",
    "    return {\n",
    "        \"auc\": (aucs.mean(),  aucs.std(ddof=1)),\n",
    "        \"acc\": (accs.mean(),  accs.std(ddof=1)),\n",
    "        \"top3_acc\": (top3s.mean(), top3s.std(ddof=1)),\n",
    "        \"macro_f1\": (f1s.mean(),  f1s.std(ddof=1)),\n",
    "    }\n",
    "\n",
    "def summarize_acc_per_dataset(all_results):\n",
    "    # assume each result has same dataset keys\n",
    "    ds_keys = list(all_results[0][\"acc_per_dataset\"].keys())\n",
    "    out = {}\n",
    "    for ds in ds_keys:\n",
    "        vals = np.array([r[\"acc_per_dataset\"][ds] for r in all_results], dtype=float)\n",
    "        out[ds] = (vals.mean(), vals.std(ddof=1))\n",
    "    return out\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TensorDatasetWithNames(Dataset):\n",
    "    def __init__(self, X, y, names):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.names = names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.names[idx]\n",
    "\n",
    "\n",
    "class TensorDatasetWithNamesComb(Dataset):\n",
    "    def __init__(self, X, X2, y, names):\n",
    "        self.X = X\n",
    "        self.X2 = X2\n",
    "        self.y = y\n",
    "        self.names = names\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.X2[idx], self.y[idx], self.names[idx]\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "print(\"✅ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDatasets[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_mel_experiment(seed: int):\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset_mel = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset_mel   = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset_mel  = TensorDatasetWithNames(\n",
    "        X_test_tensor,\n",
    "        y_test_tensor,\n",
    "        myDatasets[test_idx]\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: seed-controlled shuffling\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    train_loader_mel = DataLoader(\n",
    "        train_dataset_mel,\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        generator=g,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader_mel = DataLoader(\n",
    "        val_dataset_mel,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader_mel = DataLoader(\n",
    "        test_dataset_mel,\n",
    "        batch_size=256,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Train (re-init model every seed)\n",
    "    model_mel = MelSpectrogramCNN().to(device)\n",
    "    model_mel, history_mel = train_model(\n",
    "        model_mel,\n",
    "        train_loader_mel,\n",
    "        val_loader_mel,\n",
    "        num_epochs=60,\n",
    "        model_name=f\"model_mel_seed{seed}\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    results_mel = evaluate_model(\n",
    "        model_mel,\n",
    "        test_loader_mel,\n",
    "        f\"best_model_mel_seed{seed}.pth\",\n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(results_mel)\n",
    "\n",
    "    return results_mel\n",
    "\n",
    "\n",
    "\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "all_results = []\n",
    "\n",
    "for s in seeds:\n",
    "    print(f\"\\n{'='*20} Seed {s} {'='*20}\")\n",
    "    all_results.append(run_mel_experiment(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_with_f1(all_results)\n",
    "ds_summary = summarize_acc_per_dataset(all_results)\n",
    "print(f\"AUC: {summary['auc'][0]:.4f} ± {summary['auc'][1]:.4f}\")\n",
    "print(f\"Macro-F1: {summary['macro_f1'][0]:.4f} ± {summary['macro_f1'][1]:.4f}\")\n",
    "print(f\"Acc: {summary['acc'][0]:.4f} ± {summary['acc'][1]:.4f}\")\n",
    "print(f\"Top-3: {summary['top3_acc'][0]:.4f} ± {summary['top3_acc'][1]:.4f}\")\n",
    "for ds, (mu, sd) in ds_summary.items():\n",
    "    print(f\"{ds}: {mu:.4f} ± {sd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_topo_experiment(seed: int):\n",
    "    set_seed(seed)\n",
    "\n",
    "    train_dataset_topo = TensorDataset(X_train2_tensor, y_train_tensor)\n",
    "    val_dataset_topo   = TensorDataset(X_val2_tensor, y_val_tensor)\n",
    "    test_dataset_topo  = TensorDatasetWithNames(\n",
    "        X_test2_tensor,\n",
    "        y_test_tensor,\n",
    "        myDatasets[test_idx]\n",
    "    )\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    train_loader_topo = DataLoader(\n",
    "        train_dataset_topo,\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        generator=g,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader_topo = DataLoader(\n",
    "        val_dataset_topo,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader_topo = DataLoader(\n",
    "        test_dataset_topo,\n",
    "        batch_size=256,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Train (re-init every seed)\n",
    "    model_topo = TopologicalCNN().to(device)\n",
    "    model_topo, history_topo = train_model(\n",
    "        model_topo,\n",
    "        train_loader_topo,\n",
    "        val_loader_topo,\n",
    "        num_epochs=60,\n",
    "        lr=1e-4,\n",
    "        model_name=f\"model_topo_seed{seed}\",\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    results_topo = evaluate_model(\n",
    "        model_topo,\n",
    "        test_loader_topo,\n",
    "        f\"best_model_topo_seed{seed}.pth\",\n",
    "        device\n",
    "    )\n",
    "\n",
    "    return results_topo\n",
    "\n",
    "\n",
    "\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "all_results_topo = []\n",
    "\n",
    "for s in seeds:\n",
    "    print(f\"\\n{'='*20} TOPO Seed {s} {'='*20}\")\n",
    "    all_results_topo.append(run_topo_experiment(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_with_f1(all_results_topo)\n",
    "ds_summary = summarize_acc_per_dataset(all_results_topo)\n",
    "print(f\"AUC: {summary['auc'][0]:.4f} ± {summary['auc'][1]:.4f}\")\n",
    "print(f\"Macro-F1: {summary['macro_f1'][0]:.4f} ± {summary['macro_f1'][1]:.4f}\")\n",
    "print(f\"Acc: {summary['acc'][0]:.4f} ± {summary['acc'][1]:.4f}\")\n",
    "print(f\"Top-3: {summary['top3_acc'][0]:.4f} ± {summary['top3_acc'][1]:.4f}\")\n",
    "for ds, (mu, sd) in ds_summary.items():\n",
    "    print(f\"{ds}: {mu:.4f} ± {sd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fusion_experiment(seed: int):\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset_comb = TensorDataset(X_train_tensor, X_train2_tensor, y_train_tensor)\n",
    "    val_dataset_comb   = TensorDataset(X_val_tensor, X_val2_tensor, y_val_tensor)\n",
    "    test_dataset_comb  = TensorDatasetWithNamesComb(\n",
    "        X_test_tensor,\n",
    "        X_test2_tensor,\n",
    "        y_test_tensor,\n",
    "        myDatasets[test_idx]\n",
    "    )\n",
    "\n",
    "    # Seed-controlled shuffling\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    train_loader_comb = DataLoader(\n",
    "        train_dataset_comb,\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        generator=g,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader_comb = DataLoader(\n",
    "        val_dataset_comb,\n",
    "        batch_size=256,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_loader_comb = DataLoader(\n",
    "        test_dataset_comb,\n",
    "        batch_size=256,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Train (re-init every seed)\n",
    "    model_comb = CombinedFusionModel().to(device)\n",
    "    model_comb, history_comb = train_model(\n",
    "        model_comb,\n",
    "        train_loader_comb,\n",
    "        val_loader_comb,\n",
    "        num_epochs=60,\n",
    "        model_name=f\"model_combined_seed{seed}\",   # avoid overwrite\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Evaluate (seed-specific checkpoint)\n",
    "    results_comb = evaluate_model(\n",
    "        model_comb,\n",
    "        test_loader_comb,\n",
    "        f\"best_model_combined_seed{seed}.pth\",     # ensure train_model saves this\n",
    "        device\n",
    "    )\n",
    "\n",
    "    return results_comb\n",
    "\n",
    "\n",
    "# ---- Run multiple seeds and summarize ----\n",
    "seeds = [0, 1, 2, 3, 4]   # N=5\n",
    "all_results_comb = []\n",
    "\n",
    "for s in seeds:\n",
    "    print(f\"\\n{'='*20} FUSION Seed {s} {'='*20}\")\n",
    "    all_results_comb.append(run_fusion_experiment(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize_with_f1(all_results_comb)\n",
    "ds_summary = summarize_acc_per_dataset(all_results_comb)\n",
    "print(f\"AUC: {summary['auc'][0]:.4f} ± {summary['auc'][1]:.4f}\")\n",
    "print(f\"Macro-F1: {summary['macro_f1'][0]:.4f} ± {summary['macro_f1'][1]:.4f}\")\n",
    "print(f\"Acc: {summary['acc'][0]:.4f} ± {summary['acc'][1]:.4f}\")\n",
    "print(f\"Top-3: {summary['top3_acc'][0]:.4f} ± {summary['top3_acc'][1]:.4f}\")\n",
    "for ds, (mu, sd) in ds_summary.items():\n",
    "    print(f\"{ds}: {mu:.4f} ± {sd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretability: Integrated Gradients per Channel\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_channel_importance_ig(model, test_loader, device, save_dir=\"attr_summary\"):\n",
    "    \"\"\"\n",
    "    Compute per-channel attribution using Integrated Gradients.\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ig = IntegratedGradients(model)\n",
    "    model.eval()\n",
    "\n",
    "    acc_global = None\n",
    "    count_global = 0\n",
    "\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        baseline = torch.zeros_like(X_batch)\n",
    "\n",
    "        # Compute IG\n",
    "        attr = ig.attribute(X_batch, baselines=baseline, target=y_batch)\n",
    "        batch_attr = attr.abs().sum(dim=0).detach().cpu()  # (C, H, W)\n",
    "\n",
    "        acc_global = batch_attr if acc_global is None else acc_global + batch_attr\n",
    "        count_global += X_batch.size(0)\n",
    "\n",
    "    mean_attr_global = acc_global / max(count_global, 1)\n",
    "    C, H, W = mean_attr_global.shape\n",
    "\n",
    "    # Per-channel importance scores\n",
    "    per_channel_scores = mean_attr_global.view(C, -1).mean(dim=1).numpy()\n",
    "\n",
    "    print(\"\\nPer-Channel Importance (Integrated Gradients):\")\n",
    "    channel_names = [\n",
    "        'Euclidean (H0)',\n",
    "        'Euclidean (H1)',\n",
    "        'Temporal (H0)',\n",
    "        'Temporal (H1)',\n",
    "        'Wasserstein (H0)',\n",
    "        'Wasserstein (H1)'\n",
    "    ]\n",
    "    for c, (name, score) in enumerate(zip(channel_names, per_channel_scores)):\n",
    "        print(f\"Channel {c} ({name}): {score:.6f}\")\n",
    "\n",
    "    # Save heatmaps\n",
    "    def save_heat(heat2d, title, path):\n",
    "        h = (heat2d - heat2d.min()) / (heat2d.max() - heat2d.min() + 1e-8)\n",
    "        plt.figure(figsize=(4.5, 4))\n",
    "        plt.imshow(h, cmap=\"jet\")\n",
    "        plt.title(title)\n",
    "        plt.axis(\"off\")\n",
    "        plt.colorbar()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    for c in range(C):\n",
    "        heat = mean_attr_global[c].numpy()\n",
    "        save_heat(heat, f\"IG Attribution — {channel_names[c]}\",\n",
    "                  save_dir / f\"ig_channel_{c}.png\")\n",
    "\n",
    "    # Bar chart\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.bar(range(C), per_channel_scores)\n",
    "    plt.xlabel(\"Channel\", fontsize=12)\n",
    "    plt.ylabel(\"Mean |IG Attribution|\", fontsize=12)\n",
    "    plt.title(\"Per-Channel Importance (Integrated Gradients)\", fontsize=14)\n",
    "    plt.xticks(range(C), [f\"Ch{c}\\n{name}\" for c, name in enumerate(channel_names)],\n",
    "               rotation=45, ha='right', fontsize=9)\n",
    "\n",
    "    # Color bars by metric type\n",
    "    colors = ['#1f77b4', '#1f77b4', '#ff7f0e', '#ff7f0e', '#2ca02c', '#2ca02c']\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / \"channel_importance_ig.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    np.save(save_dir / \"per_channel_scores_ig.npy\", per_channel_scores)\n",
    "    return per_channel_scores\n",
    "\n",
    "test_dataset_topo  = TensorDataset(\n",
    "    X_test2_tensor,\n",
    "    y_test_tensor\n",
    ")\n",
    "\n",
    "test_loader_topo = DataLoader(\n",
    "    test_dataset_topo,\n",
    "    batch_size=256,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model_topo = TopologicalCNN().to(device)\n",
    "# Run IG analysis on topological\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Interpretability Analysis: Integrated Gradients\")\n",
    "print(\"=\"*60)\n",
    "model_topo.load_state_dict(torch.load(\"best_model_topo_seed0.pth\"))\n",
    "channel_scores_ig = compute_channel_importance_ig(model_topo, test_loader_topo, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occlusion analysis per channel\n",
    "def compute_channel_importance_occlusion(model, test_loader, device,\n",
    "                                         patch_size=8, stride=4,\n",
    "                                         save_dir=\"attr_summary\"):\n",
    "    \"\"\"\n",
    "    Compute per-channel importance using Occlusion method.\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    occ = Occlusion(model)\n",
    "    model.eval()\n",
    "\n",
    "    # Sample a few examples for visualization\n",
    "    X_sample, y_sample = next(iter(test_loader))\n",
    "    X_sample = X_sample[:5].to(device)  # First 5 samples\n",
    "    y_sample = y_sample[:5].to(device)\n",
    "\n",
    "    channel_names = [\n",
    "        'Euclidean (clean)',\n",
    "        'Euclidean (noise)',\n",
    "        'Temporal (clean)',\n",
    "        'Temporal (noise)',\n",
    "        'Wasserstein (clean)',\n",
    "        'Wasserstein (noise)'\n",
    "    ]\n",
    "\n",
    "    C = X_sample.shape[1]\n",
    "    attr_per_channel = []\n",
    "\n",
    "    for sample_idx in range(len(X_sample)):\n",
    "        x = X_sample[sample_idx:sample_idx+1]\n",
    "        target = int(y_sample[sample_idx])\n",
    "\n",
    "        for c in range(C):\n",
    "            attr = occ.attribute(\n",
    "                x,\n",
    "                target=target,\n",
    "                sliding_window_shapes=(1, patch_size, patch_size),\n",
    "                strides=(1, stride, stride),\n",
    "                baselines=0\n",
    "            )\n",
    "            # Extract channel c attribution\n",
    "            heat = attr[:, c:c+1, :, :].abs().mean(dim=1)[0].detach().cpu().numpy()\n",
    "            attr_per_channel.append(heat)\n",
    "\n",
    "            # Save individual sample heatmap\n",
    "            plt.figure(figsize=(5, 4))\n",
    "            plt.imshow(heat, cmap='jet')\n",
    "            plt.title(f\"Sample {sample_idx} - {channel_names[c]}\")\n",
    "            plt.colorbar()\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_dir / f\"occ_sample{sample_idx}_ch{c}.png\", dpi=150)\n",
    "            plt.close()\n",
    "\n",
    "    print(\"\\n✅ Occlusion analysis complete. Heatmaps saved to\", save_dir)\n",
    "    return attr_per_channel\n",
    "\n",
    "\n",
    "# Run Occlusion analysis\n",
    "if 'model_topo' in locals() and 'test_loader_topo' in locals():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Interpretability Analysis: Occlusion\")\n",
    "    print(\"=\"*60)\n",
    "    model_topo.load_state_dict(torch.load(\"best_model_topo_seed0.pth\"))\n",
    "    occlusion_attrs = compute_channel_importance_occlusion(\n",
    "        model_topo, test_loader_topo, device, patch_size=8, stride=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize embeddings\n",
    "def visualize_embeddings(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Extract 64-dim embeddings and project to 2D using UMAP.\n",
    "    \"\"\"\n",
    "    import umap.umap_ as umap\n",
    "\n",
    "    model.eval()\n",
    "    embs, labs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            z = model(X_batch, return_embedding=True)  # (B, 64)\n",
    "            embs.append(z.cpu().numpy())\n",
    "            labs.append(y_batch.numpy())\n",
    "\n",
    "    X_emb = np.concatenate(embs, axis=0)\n",
    "    y_emb = np.concatenate(labs, axis=0)\n",
    "\n",
    "    # UMAP projection\n",
    "    print(\"Computing UMAP projection...\")\n",
    "    X_2d = umap.UMAP(random_state=42, n_neighbors=15, min_dist=0.1).fit_transform(X_emb)\n",
    "\n",
    "    # Plot\n",
    "    emotions = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust']\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for c, (name, color) in enumerate(zip(emotions, colors)):\n",
    "        mask = (y_emb == c)\n",
    "        plt.scatter(X_2d[mask, 0], X_2d[mask, 1], s=20, alpha=0.6,\n",
    "                    label=name, color=color)\n",
    "\n",
    "    plt.title(\"UMAP Projection of Topological Embeddings\", fontsize=14)\n",
    "    plt.xlabel(\"UMAP 1\", fontsize=12)\n",
    "    plt.ylabel(\"UMAP 2\", fontsize=12)\n",
    "    plt.legend(loc='best', fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('embedding_umap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"\\n✅ Embedding visualization saved: embedding_umap.png\")\n",
    "    return X_2d, y_emb\n",
    "\n",
    "\n",
    "# Visualize topological embeddings\n",
    "if 'model_topo' in locals() and 'test_loader_topo' in locals():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Embedding Visualization\")\n",
    "    print(\"=\"*60)\n",
    "    model_topo.load_state_dict(torch.load(\"best_model_topo_seed0.pth\"))\n",
    "    X_2d, y_2d = visualize_embeddings(model_topo, test_loader_topo, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleTopologicalCNN(nn.Module):\n",
    "    def __init__(self, num_channels=6, num_classes=6):\n",
    "        super(FlexibleTopologicalCNN, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_ablation_model(X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "                         channel_indices, model_name, device,\n",
    "                         num_epochs=30, lr=1e-3, verbose=False):\n",
    "\n",
    "    X_train_sub = X_train[:, channel_indices, :, :]\n",
    "    X_val_sub = X_val[:, channel_indices, :, :]\n",
    "    X_test_sub = X_test[:, channel_indices, :, :]\n",
    "\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_sub, y_train)\n",
    "    val_dataset = TensorDataset(X_val_sub, y_val)\n",
    "    test_dataset = TensorDataset(X_test_sub, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "    model = FlexibleTopologicalCNN(num_channels=len(channel_indices)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    auroc = MulticlassAUROC(num_classes=6, average='macro').to(device)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                preds = torch.softmax(outputs, dim=1)\n",
    "                val_preds.append(preds)\n",
    "                val_labels.append(y_batch)\n",
    "\n",
    "        val_preds = torch.cat(val_preds)\n",
    "        val_labels = torch.cat(val_labels)\n",
    "\n",
    "        val_auc = auroc(val_preds, val_labels).item()\n",
    "        y_pred = torch.argmax(val_preds, dim=1)\n",
    "        accuracy = (y_pred == val_labels).float().mean().item()\n",
    "\n",
    "\n",
    "        if accuracy > best_val_acc:\n",
    "            best_val_acc = accuracy\n",
    "            best_state = model.state_dict().copy()\n",
    "\n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{num_epochs} - val_auc: {val_auc:.4f}\")\n",
    "\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "\n",
    "    test_preds, test_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = torch.softmax(outputs, dim=1)\n",
    "            test_preds.append(preds)\n",
    "            test_labels.append(y_batch)\n",
    "\n",
    "    test_preds = torch.cat(test_preds)\n",
    "    test_labels = torch.cat(test_labels)\n",
    "\n",
    "    test_auc = auroc(test_preds, test_labels).item()\n",
    "    test_acc = (torch.argmax(test_preds, dim=1) == test_labels).float().mean().item()\n",
    "\n",
    "    return {\n",
    "        'auc': test_auc,\n",
    "        'acc': test_acc,\n",
    "        'val_acc': best_val_acc\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✅ Ablation training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Run ablation experiments\n",
    "if 'X_train2_tensor' in locals():\n",
    "    print(\"=\"*60)\n",
    "    print(\"Running Ablation Study\")\n",
    "    print(\"=\"*60)\n",
    "    SEEDS = [0, 1, 2, 3, 4]\n",
    "\n",
    "    # Define ablation configurations\n",
    "    ablation_configs = {\n",
    "        # Baseline\n",
    "        # 'All 6 channels': [0, 1, 2, 3, 4, 5],\n",
    "\n",
    "        # Individual channels\n",
    "        'Euclidean H0': [0],\n",
    "        'Euclidean H1': [1],\n",
    "        'Temporal H0': [2],\n",
    "        'Temporal H1': [3],\n",
    "        'Wasserstein H0': [4],\n",
    "        'Wasserstein H1': [5],\n",
    "\n",
    "        # Metric groups\n",
    "        'Euclidean only': [0, 1],\n",
    "        'Temporal only': [2, 3],\n",
    "        'Wasserstein only': [4, 5],\n",
    "\n",
    "        # Leave-one-out (remove metric groups)\n",
    "        'Without Euclidean': [2, 3, 4, 5],\n",
    "        'Without Temporal': [0, 1, 4, 5],\n",
    "        'Without Wasserstein': [0, 1, 2, 3],\n",
    "    }\n",
    "\n",
    "    ablation_results = {}\n",
    "\n",
    "    ablation_results = {}\n",
    "\n",
    "for config_name, channels in ablation_configs.items():\n",
    "    print(f\"\\n{config_name} (channels {channels})...\")\n",
    "\n",
    "    seed_results = []\n",
    "\n",
    "    for seed in tqdm(SEEDS):\n",
    "            # Set seeds\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "            results = train_ablation_model(\n",
    "                X_train2_tensor, X_val2_tensor, X_test2_tensor,\n",
    "                y_train_tensor, y_val_tensor, y_test_tensor,\n",
    "                channel_indices=channels,\n",
    "                model_name=f\"ablation_{config_name.replace(' ', '_')}_seed{seed}\",\n",
    "                device=device,\n",
    "                num_epochs=60,\n",
    "                lr=1e-4,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            seed_results.append(results)\n",
    "\n",
    "    # Aggregate metrics\n",
    "    aucs = np.array([r['auc'] for r in seed_results], dtype=float)\n",
    "    accs = np.array([r['acc'] for r in seed_results], dtype=float)\n",
    "\n",
    "    ablation_results[config_name] = {\n",
    "        'auc_mean': aucs.mean(),\n",
    "        'auc_std': aucs.std(ddof=1),\n",
    "        'acc_mean': accs.mean(),\n",
    "        'acc_std': accs.std(ddof=1),\n",
    "        'num_seeds': len(SEEDS)\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"  ✅ Test AUC: {aucs.mean():.4f} ± {aucs.std(ddof=1):.4f} | \"\n",
    "        f\"Test Acc: {accs.mean():.4f} ± {accs.std(ddof=1):.4f}\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Ablation Study Complete (all configs, multi-seed)!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Training data not available. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret_persistence_images.py\n",
    "# Requirements:\n",
    "#   pip install captum matplotlib numpy torch\n",
    "#\n",
    "# Assumptions:\n",
    "#   - Your model: model(pi) -> logits [B, num_classes]\n",
    "#   - Your dataloader yields: (pi, label, *optional_metadata)\n",
    "#       pi: float tensor [B, 6, 32, 32]\n",
    "#       label: long tensor [B]\n",
    "#\n",
    "# If you have a fusion model, wrap it so that forward(pi) returns logits.\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "CHANNEL_NAMES = [\n",
    "    \"euclid_H0\", \"euclid_H1\",\n",
    "    \"temporal_H0\", \"temporal_H1\",\n",
    "    \"wasserstein_H0\", \"wasserstein_H1\",\n",
    "]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IGResults:\n",
    "    # Means are normalized by sample count\n",
    "    mean_abs_attr: torch.Tensor        # [C,H,W]\n",
    "    mean_abs_attr_by_class: torch.Tensor  # [K,C,H,W]\n",
    "    mean_pi_by_class: torch.Tensor        # [K,C,H,W]\n",
    "    mean_abs_attr_channel: torch.Tensor   # [C]\n",
    "    mean_abs_attr_channel_by_class: torch.Tensor  # [K,C]\n",
    "    counts_by_class: torch.Tensor         # [K]\n",
    "    num_samples: int\n",
    "    num_classes: int\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _infer_num_classes(model: nn.Module, device: torch.device, c=6, h=32, w=32) -> int:\n",
    "    x = torch.zeros(1, c, h, w, device=device)\n",
    "    y = model(x)\n",
    "    return int(y.shape[-1])\n",
    "\n",
    "\n",
    "def compute_integrated_gradients(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    num_classes: Optional[int] = None,\n",
    "    n_steps: int = 64,\n",
    "    target_mode: str = \"true\",   # \"true\" or \"pred\"\n",
    "    max_batches: Optional[int] = None,\n",
    ") -> IGResults:\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    if num_classes is None:\n",
    "        num_classes = _infer_num_classes(model, device)\n",
    "\n",
    "    ig = IntegratedGradients(model)\n",
    "\n",
    "    # Running sums\n",
    "    sum_abs_attr = torch.zeros(6, 32, 32, device=device)\n",
    "    sum_abs_attr_by_class = torch.zeros(num_classes, 6, 32, 32, device=device)\n",
    "    sum_pi_by_class = torch.zeros(num_classes, 6, 32, 32, device=device)\n",
    "\n",
    "    sum_abs_attr_channel = torch.zeros(6, device=device)\n",
    "    sum_abs_attr_channel_by_class = torch.zeros(num_classes, 6, device=device)\n",
    "\n",
    "    counts_by_class = torch.zeros(num_classes, device=device)\n",
    "    total_samples = 0\n",
    "\n",
    "    for b_idx, batch in enumerate(dataloader):\n",
    "        if max_batches is not None and b_idx >= max_batches:\n",
    "            break\n",
    "\n",
    "        # Accept (pi, label) or (pi, label, meta...)\n",
    "        pi = batch[0].to(device).float()\n",
    "        y = batch[1].to(device).long()\n",
    "        bsz = pi.shape[0]\n",
    "\n",
    "        # Choose attribution target\n",
    "        with torch.no_grad():\n",
    "            logits = model(pi)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "\n",
    "        if target_mode == \"true\":\n",
    "            target = y\n",
    "        elif target_mode == \"pred\":\n",
    "            target = pred\n",
    "\n",
    "        pi_req = pi.detach().clone().requires_grad_(True)\n",
    "        baseline = torch.zeros_like(pi_req)\n",
    "\n",
    "        attr = ig.attribute(\n",
    "            inputs=pi_req,\n",
    "            baselines=baseline,\n",
    "            target=target,\n",
    "            n_steps=n_steps,\n",
    "            internal_batch_size=min(32, bsz),\n",
    "        )\n",
    "\n",
    "        abs_attr = attr.abs()\n",
    "\n",
    "        sum_abs_attr += abs_attr.sum(dim=0) \n",
    "        sum_abs_attr_channel += abs_attr.sum(dim=(0, 2, 3)) \n",
    "\n",
    "        for k in range(num_classes):\n",
    "            mask = (y == k)\n",
    "            if mask.any():\n",
    "                m = mask.nonzero(as_tuple=False).squeeze(1)\n",
    "                counts_by_class[k] += float(m.numel())\n",
    "                sum_abs_attr_by_class[k] += abs_attr[m].sum(dim=0)\n",
    "                sum_pi_by_class[k] += pi[m].sum(dim=0)\n",
    "                sum_abs_attr_channel_by_class[k] += abs_attr[m].sum(dim=(0, 2, 3))\n",
    "\n",
    "        total_samples += bsz\n",
    "\n",
    "\n",
    "    mean_abs_attr = sum_abs_attr / max(total_samples, 1)\n",
    "    mean_abs_attr_channel = sum_abs_attr_channel / max(total_samples * 32 * 32, 1)\n",
    "\n",
    "    mean_abs_attr_by_class = torch.zeros_like(sum_abs_attr_by_class)\n",
    "    mean_pi_by_class = torch.zeros_like(sum_pi_by_class)\n",
    "    mean_abs_attr_channel_by_class = torch.zeros_like(sum_abs_attr_channel_by_class)\n",
    "\n",
    "    for k in range(num_classes):\n",
    "        ck = int(counts_by_class[k].item())\n",
    "        if ck > 0:\n",
    "            mean_abs_attr_by_class[k] = sum_abs_attr_by_class[k] / ck\n",
    "            mean_pi_by_class[k] = sum_pi_by_class[k] / ck\n",
    "            mean_abs_attr_channel_by_class[k] = sum_abs_attr_channel_by_class[k] / (ck * 32 * 32)\n",
    "\n",
    "    return IGResults(\n",
    "        mean_abs_attr=mean_abs_attr.detach().cpu(),\n",
    "        mean_abs_attr_by_class=mean_abs_attr_by_class.detach().cpu(),\n",
    "        mean_pi_by_class=mean_pi_by_class.detach().cpu(),\n",
    "        mean_abs_attr_channel=mean_abs_attr_channel.detach().cpu(),\n",
    "        mean_abs_attr_channel_by_class=mean_abs_attr_channel_by_class.detach().cpu(),\n",
    "        counts_by_class=counts_by_class.detach().cpu(),\n",
    "        num_samples=total_samples,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_channel_importance(mean_abs_attr_channel: torch.Tensor, title: str = \"Per-channel IG importance\"):\n",
    "    vals = mean_abs_attr_channel.numpy()\n",
    "    x = np.arange(len(vals))\n",
    "    plt.figure()\n",
    "    plt.bar(x, vals)\n",
    "    plt.xticks(x, CHANNEL_NAMES, rotation=35, ha=\"right\")\n",
    "    plt.ylabel(\"Mean |IG attribution| per pixel\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_attr_heatmaps(mean_abs_attr: torch.Tensor, max_channels: int = 6, title_prefix: str = \"Mean |IG|\"):\n",
    "    C = min(mean_abs_attr.shape[0], max_channels)\n",
    "    for c in range(C):\n",
    "        plt.figure()\n",
    "        plt.imshow(mean_abs_attr[c].numpy(), origin=\"lower\", aspect=\"auto\")\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"{title_prefix}: {CHANNEL_NAMES[c]}\")\n",
    "        plt.xlabel(\"PI x-bin\")\n",
    "        plt.ylabel(\"PI y-bin\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_class_prototypes(\n",
    "    mean_pi_by_class: torch.Tensor,\n",
    "    mean_abs_attr_by_class: torch.Tensor,\n",
    "    class_names: List[str],\n",
    "    class_idx: int,\n",
    "    channels: Optional[List[int]] = None,\n",
    "):\n",
    "    if channels is None:\n",
    "        channels = list(range(mean_pi_by_class.shape[1]))\n",
    "\n",
    "    for c in channels:\n",
    "        plt.figure()\n",
    "        plt.imshow(mean_pi_by_class[class_idx, c].numpy(), origin=\"lower\", aspect=\"auto\")\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"Mean PI prototype — class={class_names[class_idx]} — {CHANNEL_NAMES[c]}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(mean_abs_attr_by_class[class_idx, c].numpy(), origin=\"lower\", aspect=\"auto\")\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"Mean |IG| — class={class_names[class_idx]} — {CHANNEL_NAMES[c]}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_difference_map(\n",
    "    tensor_by_class: torch.Tensor,\n",
    "    class_names: List[str],\n",
    "    a: int,\n",
    "    b: int,\n",
    "    channel: int,\n",
    "    title: str = \"Difference map\",\n",
    "):\n",
    "    diff = tensor_by_class[a, channel] - tensor_by_class[b, channel]\n",
    "    plt.figure()\n",
    "    plt.imshow(diff.numpy(), origin=\"lower\", aspect=\"auto\",\n",
    "           vmin=200, vmax=-200)\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"{title}: {class_names[a]} - {class_names[b]} — {CHANNEL_NAMES[channel]}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_topo = TopologicalCNN().to(device)\n",
    "model_topo.load_state_dict(torch.load(\"best_model_topo_seed4.pth\"))\n",
    "\n",
    "test_dataset_topo  = TensorDataset(\n",
    "    X_test2_tensor,\n",
    "    y_test_tensor\n",
    ")\n",
    "\n",
    "test_loader_topo = DataLoader(\n",
    "    test_dataset_topo,\n",
    "    batch_size=256,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "results = compute_integrated_gradients(\n",
    "    model=model_topo,\n",
    "    dataloader=test_loader_topo,\n",
    "    device=device,\n",
    "    num_classes=6,\n",
    "    n_steps=64,\n",
    "    target_mode=\"true\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"neutral\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\"]  # adjust if needed\n",
    "\n",
    "# plot_difference_map(results.mean_pi_by_class, class_names, a=4, b=0, channel=2, title=\"PI prototype diff\")\n",
    "# plot_difference_map(results.mean_pi_by_class, class_names, a=2, b=0, channel=2, title=\"PI prototype diff\")\n",
    "# plot_difference_map(results.mean_pi_by_class, class_names, a=5, b=0, channel=2, title=\"PI prototype diff\")\n",
    "\n",
    "plot_difference_map(results.mean_abs_attr_by_class, class_names, a=4, b=0, channel=2, title=\"PI prototype diff\")\n",
    "plot_difference_map(results.mean_abs_attr_by_class, class_names, a=2, b=0, channel=2, title=\"PI prototype diff\")\n",
    "plot_difference_map(results.mean_abs_attr_by_class, class_names, a=5, b=0, channel=2, title=\"PI prototype diff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
