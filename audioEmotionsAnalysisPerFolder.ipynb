{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_requirements_file(file_path=\"requirements.txt\"):\n",
    "#     \"\"\"\n",
    "#     Reads a requirements.txt file and prints each line.\n",
    "    \n",
    "#     Args:\n",
    "#         file_path (str): The path to the requirements.txt file.\n",
    "#     \"\"\"\n",
    "#     req = []\n",
    "#     try:\n",
    "#         with open(file_path, 'r') as f:\n",
    "#             for line in f:\n",
    "#                 # Remove leading/trailing whitespace and print the line\n",
    "#                 req.append(line.strip())\n",
    "        \n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Error: The file '{file_path}' was not found.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")\n",
    "#     return req\n",
    "\n",
    "# # Example usage:\n",
    "# read_requirements_file() \n",
    "# # If your requirements.txt is in a different location, specify the path:\n",
    "# # read_requirements_file(\"path/to/your/requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import aiplatform\n",
    "\n",
    "# PROJECT_ID = \"strange-metrics-266115\"\n",
    "# LOCATION = \"us-central1\"\n",
    "# STAGING_BUCKET = \"gs://simplicialcomplex-inputdata\"  # your bucket\n",
    "\n",
    "# aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./strange-metrics-266115-d7ca05a26868.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import storage\n",
    "\n",
    "# def upload_to_gcs(bucket_name, source_file, destination_blob):\n",
    "#         \"\"\"Upload a file to a GCS bucket.\"\"\"\n",
    "#         client = storage.Client()\n",
    "#         bucket = client.bucket(bucket_name)\n",
    "#         blob = bucket.blob(destination_blob)\n",
    "#         blob.upload_from_filename(source_file)\n",
    "#         print(f\"✅ Uploaded {destination_blob} to gs://{bucket_name}/{destination_blob}\")\n",
    "# GCS_BUCKET = \"simplicialcomplex-outputbucket\"\n",
    "\n",
    "# upload_to_gcs(GCS_BUCKET, \"myData.npz\", \"data/myData.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with np.load(\"myData.npz\") as data:\n",
    "#     myData = data['myData']\n",
    "#     myData2 = data['myData2']\n",
    "#     myY = data['myY']\n",
    "#     myActors = data['myActors']\n",
    "#     print(np.unique(myY))\n",
    "#     print(np.unique(myActors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import storage\n",
    "\n",
    "# # Initialize client\n",
    "# client = storage.Client(project=\"strange-metrics-266115\")\n",
    "\n",
    "# # Reference the bucket\n",
    "# bucket_name = \"simplicialcomplex-inputdata\"\n",
    "# bucket = client.bucket(bucket_name)\n",
    "\n",
    "# # List blobs and count\n",
    "# blobs = client.list_blobs(bucket_name, prefix='savefiles')\n",
    "# count = sum(1 for _ in blobs)\n",
    "# print(f\"Number of objects in bucket '{bucket_name}': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import aiplatform\n",
    "\n",
    "# aiplatform.init(project=\"strange-metrics-266115\", location=\"us-central1\")\n",
    "\n",
    "# job = aiplatform.CustomJob.from_local_script(\n",
    "#     display_name=\"my-tf-job\",\n",
    "#     script_path=\"fitPersistencesCombinedModelTorchCloud.py\",\n",
    "#     container_uri=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-7:latest\",\n",
    "#     staging_bucket=\"gs://simplicialcomplex-outputbucket\",\n",
    "#     requirements=[\"seaborn\", \"torchmetrics\"],\n",
    "#     machine_type=\"n1-standard-4\",\n",
    "#     accelerator_type=\"NVIDIA_TESLA_T4\", \n",
    "#     accelerator_count=1 \n",
    "# )\n",
    "\n",
    "# job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import re\n",
    "from scipy.io import wavfile\n",
    "from scipy.fftpack import fft\n",
    "import cv2\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from glob import glob\n",
    "import skimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "# import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint,LearningRateScheduler\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gudhi import RipsComplex\n",
    "import gudhi as gd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import wasserstein_distance\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gudhi.representations import Landscape\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "from gudhi.representations import PersistenceImage \n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from multiprocessing import cpu_count\n",
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import which\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin\"\n",
    "AudioSegment.converter = \"/opt/homebrew/bin/ffmpeg\"\n",
    "AudioSegment.ffprobe = \"/opt/homebrew/bin/ffprobe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path_radvess = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path_radvess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path_cremad = kagglehub.dataset_download(\"ejlok1/cremad\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path_cremad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path_savee = kagglehub.dataset_download(\"ejlok1/surrey-audiovisual-expressed-emotion-savee\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path_savee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path_tess = kagglehub.dataset_download(\"ejlok1/toronto-emotional-speech-set-tess\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path_tess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(sound: AudioSegment, noise_level=0.02):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to a pydub AudioSegment.\n",
    "    noise_level: fraction of max amplitude (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    samples = np.array(sound.get_array_of_samples()).astype(np.float32)\n",
    "    # Normalize to [-1, 1]\n",
    "    samples /= np.iinfo(sound.array_type).max\n",
    "    # Add Gaussian noise\n",
    "    noise = np.random.normal(0, noise_level, size=samples.shape)\n",
    "    noisy_samples = samples + noise\n",
    "    # Clip to [-1, 1]\n",
    "    noisy_samples = np.clip(noisy_samples, -1.0, 1.0)\n",
    "    # Convert back to original dtype\n",
    "    noisy_samples = (noisy_samples * np.iinfo(sound.array_type).max).astype(sound.array_type)\n",
    "    noisy_sound = sound._spawn(noisy_samples.tobytes())\n",
    "    return noisy_sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPath_radvess = 'radvess/1/'\n",
    "myEmotionMapRadvess = {\n",
    "    1: 'neutral', 2: 'calm', 3: 'happy', 4: 'sad', 5: 'angry', 6: 'fearful', 7: 'disgust', 8: 'surprised'\n",
    "}\n",
    "def loadFiles_radvess(aDirectory):\n",
    "    myEmotionsSounds, myActors, myEmotions = [], [], []\n",
    "    for myFolder in tqdm(os.listdir(aDirectory)):\n",
    "        for myFile in tqdm(os.listdir(aDirectory + myFolder)):\n",
    "            if myFile.endswith(\".wav\"):\n",
    "                myEmotion = myEmotionMapRadvess[int(myFile.split('-')[2])]\n",
    "                for i in range(2):\n",
    "                    if myEmotion != 'calm' and myEmotion != 'surprised':\n",
    "                        sound = AudioSegment.from_file(os.path.join(aDirectory + myFolder, myFile))\n",
    "                        if i % 2 == 0:\n",
    "                            sound = add_noise(sound, noise_level=0.2)\n",
    "                        myEmotionsSounds.append(sound)\n",
    "                        myActors.append(myFolder)\n",
    "                        myEmotions.append(myEmotion)\n",
    "    return myEmotionsSounds, myActors, myEmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPath_cremad = 'cremad/1/AudioWAV/'\n",
    "myEmotionMapCremad = {\n",
    "    'NEU': 'neutral', 'HAP': 'happy', 'SAD': 'sad', 'ANG': 'angry', 'FEA': 'fearful', 'DIS': 'disgust'\n",
    "}\n",
    "def loadFiles_cremad(aDirectory):\n",
    "    myEmotionsSounds, myActors, myEmotions = [], [], []\n",
    "    for myFile in tqdm(os.listdir(aDirectory)):\n",
    "        if myFile.endswith(\".wav\"):\n",
    "            myEmotion = myEmotionMapCremad[(myFile.split('_')[2].split('.')[0])]\n",
    "            if myEmotion != 'calm' and myEmotion != 'surprised':\n",
    "                for i in range(2):\n",
    "                    myActor = myFile.split('_')[0]\n",
    "                    sound = AudioSegment.from_file(os.path.join(aDirectory, myFile))\n",
    "                    if i % 2 == 0:\n",
    "                        sound = add_noise(sound, noise_level=0.2)\n",
    "                    myEmotionsSounds.append(sound)\n",
    "                    myActors.append(myActor)\n",
    "                    myEmotions.append(myEmotion)\n",
    "    return myEmotionsSounds, myActors, myEmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPath_savee = 'savee/1/ALL/'\n",
    "myEmotionMapSavee = {\n",
    "    'n': 'neutral', 'h': 'happy', 'sa': 'sad', 'a': 'angry', 'f': 'fearful', 'd': 'disgust', 'su': 'surprised'\n",
    "}\n",
    "def loadFiles_savee(aDirectory):\n",
    "    myEmotionsSounds, myActors, myEmotions = [], [], []\n",
    "    for myFile in tqdm(os.listdir(aDirectory)):\n",
    "        if myFile.endswith(\".wav\"):\n",
    "            myEmotion = myEmotionMapSavee[(myFile.split('_')[1].split('.')[0])[:-2]]\n",
    "            if myEmotion != 'calm' and myEmotion != 'surprised':\n",
    "                for i in range(2):\n",
    "                    myActor = myFile.split('_')[0]\n",
    "                    sound = AudioSegment.from_file(os.path.join(aDirectory, myFile))\n",
    "                    if i % 2 == 0:\n",
    "                        sound = add_noise(sound, noise_level=0.2)\n",
    "                    myEmotionsSounds.append(sound)\n",
    "                    myActors.append(myActor)\n",
    "                    myEmotions.append(myEmotion)\n",
    "    return myEmotionsSounds, myActors, myEmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPath_tess = 'tess/1/TESS Toronto emotional speech set data/'\n",
    "myEmotionMapTess = {\n",
    "    'neutral': 'neutral', 'happy': 'happy', 'sad': 'sad', 'angry': 'angry', 'fear': 'fearful', 'disgust': 'disgust', 'pleasant': 'surprised', 'pleasant_surprised': 'surprised'\n",
    "}\n",
    "def loadFiles_tess(aDirectory):\n",
    "    myEmotionsSounds, myActors, myEmotions = [], [], []\n",
    "    for myFolder in tqdm(os.listdir(aDirectory)):\n",
    "        if len(myFolder.split('_')) >= 2:\n",
    "            myActor = myFolder.split('_')[0]\n",
    "            myEmotion = myEmotionMapTess[myFolder.split('_')[1].lower()]\n",
    "            if myEmotion != 'calm' and myEmotion != 'surprised':\n",
    "                for myFile in tqdm(os.listdir(aDirectory + myFolder)): \n",
    "                    if myFile.endswith(\".wav\"):\n",
    "                        for i in range(2):\n",
    "                            sound = AudioSegment.from_file(os.path.join(aDirectory + myFolder, myFile))\n",
    "                            if i % 2 == 0:\n",
    "                                sound = add_noise(sound, noise_level=0.2)\n",
    "                            myEmotionsSounds.append(sound)\n",
    "                            myActors.append(myActor)\n",
    "                            myEmotions.append(myEmotion)\n",
    "    return myEmotionsSounds, myActors, myEmotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWaveform(aPath, aPathName):\n",
    "    mySignal, mySampleRate = librosa.load(aPath)\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    librosa.display.waveshow(mySignal, sr=mySampleRate)\n",
    "    plt.title(F'The {aPathName} Sound Waveform')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "    print(F\"The {aPathName} Sound: \")\n",
    "    sound = ipd.Audio(mySignal, rate=mySampleRate)\n",
    "    return sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doMelSpectrogram(aSoundFiles, aFixLength = False):\n",
    "    mySampleRate = 44100  \n",
    "    myMelSpectrograms = []\n",
    "    for i in tqdm(range(len(aSoundFiles))):\n",
    "        myAudioSegment = aSoundFiles[i]\n",
    "        myTempFile = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
    "        myTempFilePath = myTempFile.name\n",
    "        myTempFile.close()\n",
    "        myAudioSegment.export(myTempFilePath, format=\"wav\")\n",
    "        mySignal, mySampleRate = librosa.load(myTempFilePath, sr=mySampleRate)\n",
    "        os.remove(myTempFilePath)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=mySignal, sr=mySampleRate, n_mels=128) \n",
    "        mel_spectrogram_dbs = librosa.amplitude_to_db(mel_spectrogram, ref=np.max)\n",
    "        if aFixLength:\n",
    "            mel_spectrogram_dbs = librosa.util.fix_length(mel_spectrogram_dbs, size=256, axis=1)\n",
    "        myMelSpectrograms.append(mel_spectrogram_dbs)\n",
    "    return myMelSpectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMfccs(aDbs, aNumMfcc=13):\n",
    "    myMfccList = []\n",
    "    for i in tqdm(range(len(aDbs))):\n",
    "        myMfcc = librosa.feature.mfcc(S=aDbs[i], n_mfcc=aNumMfcc)\n",
    "        myMfccList.append(myMfcc)\n",
    "    return myMfccList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMelSpectrogram(aMelSpectrogram):  \n",
    "    myDfMelSpectrogram = pd.DataFrame(aMelSpectrogram)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.heatmap(myDfMelSpectrogram, cmap='plasma', xticklabels=10, yticklabels=10)\n",
    "    plt.title(f'Mel Spectrogram from an angry sound  (dB)')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Mel Frequency Bin')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showMfcc(aMfcc):\n",
    "    librosa.display.specshow(aMfcc, x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title('MFCC From an Angry Sound')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show3DPlot(aMelSpectrogram):\n",
    "    fig = go.Figure(data=[\n",
    "        go.Surface(z=aMelSpectrogram.T, colorscale='viridis')\n",
    "    ])\n",
    "    fig.update_layout(\n",
    "        title='Mel Spectrogram from and angry sound 3-D plot (dB)',\n",
    "        scene=dict(\n",
    "            xaxis=dict(title='Time (s)'),\n",
    "            yaxis=dict(title='Mel Frequency Bin'),\n",
    "            zaxis=dict(title='Amplitude (dB)')\n",
    "        )\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Getting MFCCs and Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRadvessSounds, myRadvessActors, myRadvessEmotions = loadFiles_radvess(myPath_radvess)\n",
    "myCremadSounds, myCremadActors, myCremadEmotions = loadFiles_cremad(myPath_cremad)\n",
    "mySaveeSounds, mySaveeActors, mySaveeEmotions = loadFiles_savee(myPath_savee)\n",
    "myTessSounds, myTessActors, myTessEmotions = loadFiles_tess(myPath_tess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myMelSpectrogramsRadvess = doMelSpectrogram(myRadvessSounds)\n",
    "myMelSpectrogramsCremad = doMelSpectrogram(myCremadSounds)\n",
    "myMelSpectrogramsSavee = doMelSpectrogram(mySaveeSounds)\n",
    "myMelSpectrogramsTess = doMelSpectrogram(myTessSounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myMfccRadvess = computeMfccs(myMelSpectrogramsRadvess)\n",
    "myMfccCremad = computeMfccs(myMelSpectrogramsCremad)\n",
    "myMfccSavee = computeMfccs(myMelSpectrogramsSavee)\n",
    "myMfccTess = computeMfccs(myMelSpectrogramsTess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Homologies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanMetric(x, y):\n",
    "    return np.linalg.norm(x - y) \n",
    "def timeBasedEuclideanMetric(x, y):\n",
    "    return np.linalg.norm(x[0] - y[0]) / (2) + np.linalg.norm(x[1:] - y[1:]) / (2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computerPersistenceDiagrams(aMatrix, aMaxLength, aMaxDim, aMetric, aPlot, aNormalize):\n",
    "    # Preprocess matrix\n",
    "    if aNormalize:\n",
    "        myTimeRow = np.arange(aMatrix.shape[1]) \n",
    "        myNormalizedMatrix = (aMatrix - np.min(aMatrix)) / (np.max(aMatrix) - np.min(aMatrix))\n",
    "        myTimeRow = myTimeRow / aMatrix.shape[1]\n",
    "        myNewArr = np.vstack([myTimeRow, myNormalizedMatrix])\n",
    "    else:\n",
    "        myNewArr = aMatrix\n",
    "    \n",
    "    # Compute distance matrix\n",
    "    myDistanceMatrix = squareform(pdist(myNewArr.T, metric=aMetric))\n",
    "    myMaxLength = min(np.abs(myDistanceMatrix).max().max() / 3, aMaxLength)\n",
    "\n",
    "    # Compute persistences\n",
    "    myRipsComplex = RipsComplex(points=myNewArr.T, max_edge_length=myMaxLength, distance_matrix=myDistanceMatrix)\n",
    "    mySimplexTree = myRipsComplex.create_simplex_tree(max_dimension=aMaxDim)\n",
    "    mySimplexTree.compute_persistence()\n",
    "    myPersistence = mySimplexTree.persistence()\n",
    "\n",
    "    # Compute bd matrix\n",
    "    myBd = []\n",
    "    for i in range(aMaxDim):\n",
    "        myBdMatrix = np.array([\n",
    "            [birth, death] for dim, (birth, death) in myPersistence if dim == i and death != float('inf')\n",
    "        ])\n",
    "        myBd.append(myBdMatrix)\n",
    "\n",
    "    # Plot if needed\n",
    "    if aPlot:\n",
    "        gd.plot_persistence_diagram(mySimplexTree.persistence())\n",
    "\n",
    "    return myBd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computerPersistenceDiagramsStar(args):\n",
    "    return computerPersistenceDiagrams(*args)\n",
    "def computePersistanceFromDb(aDb, aMaxLength, aMaxDim, aMetric, aPlot, aMaxDiagrams, aNormalize):\n",
    "    myMat = []\n",
    "    args = []\n",
    "    for i, myCurr in enumerate(aDb):\n",
    "        if i == aMaxDiagrams:\n",
    "            break\n",
    "        args.append((myCurr, aMaxLength, aMaxDim, aMetric, aPlot, aNormalize))\n",
    "    \n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        myMat = list(\n",
    "            tqdm(\n",
    "                pool.imap(computerPersistenceDiagramsStar, args),\n",
    "                total=len(args),\n",
    "                desc=\"Computing persistences\"\n",
    "            )\n",
    "        )\n",
    "    return myMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getPersistenceHeatMapSingle(myBdMatrix, aMax, aDim, aPlot, aShape):\n",
    "    # print(myBdMatrix)\n",
    "    myPersistenceImage = PersistenceImage(\n",
    "        bandwidth= aMax / 50,\n",
    "        weight=lambda x: np.sqrt((x[1] - x[0])**2), \n",
    "        resolution=[aShape, aShape],    \n",
    "        im_range=[0, aMax, 0, aMax]   \n",
    "    )\n",
    "\n",
    "    # If any persistence bd matrix found then compute heatmap\n",
    "    if aDim < len(myBdMatrix) and len(myBdMatrix[aDim]) > 0:\n",
    "        myHeatMap = myPersistenceImage.fit_transform([myBdMatrix[aDim]])[0]\n",
    "        myHeatMap2d = myHeatMap.reshape(aShape, aShape)\n",
    "        if aPlot:\n",
    "            print(myHeatMap2d.shape)\n",
    "            plt.figure()\n",
    "            plt.imshow(myHeatMap2d, cmap='hot', origin='lower')\n",
    "            plt.colorbar(label='Scaled Density')\n",
    "            plt.title('Persistence Image')\n",
    "            plt.xlabel('Birth')\n",
    "            plt.ylabel('Persistence')\n",
    "            plt.show()\n",
    "        return myHeatMap2d\n",
    "    else:\n",
    "        return np.zeros((aShape, aShape))\n",
    "        \n",
    "def getPersistenceHeatMapSingleStar(args):\n",
    "    return getPersistenceHeatMapSingle(*args)\n",
    "\n",
    "def getPersitanceHeatMap(aArr, aMax, aMaxDim, aPlot = False, aShape = 32):\n",
    "    myHeatmapArr = []\n",
    "    args = []\n",
    "    for i, myCurr in enumerate(aArr):\n",
    "        for j in range(aMaxDim):\n",
    "            args.append((myCurr, aMax, j, aPlot, aShape))\n",
    "    \n",
    "    with Pool(processes=cpu_count()) as pool:\n",
    "        myHeatmapArr = list(\n",
    "            tqdm(\n",
    "                pool.imap(getPersistenceHeatMapSingleStar, args),\n",
    "                total=len(args),\n",
    "                desc=\"Computing persistences heat map\"\n",
    "            )\n",
    "        )\n",
    "    return myHeatmapArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPersistenceT = computerPersistenceDiagrams(aMatrix=myMelSpectrogramsCremad[0], aMaxLength=1.2, aMaxDim=2, aMetric=timeBasedEuclideanMetric, aPlot=True, aNormalize=True)\n",
    "myPersistenceW = computerPersistenceDiagrams(aMatrix=myMelSpectrogramsRadvess[0], aMaxLength=2.5, aMaxDim=2, aMetric=wasserstein_distance, aPlot=True, aNormalize=False)\n",
    "myPersistenceE = computerPersistenceDiagrams(aMatrix=myMelSpectrogramsSavee[0], aMaxLength=100, aMaxDim=2, aMetric=euclideanMetric, aPlot=True, aNormalize=False)\n",
    "myPersistenceWM = computerPersistenceDiagrams(aMatrix=myMfccTess[0], aMaxLength=15, aMaxDim=2, aMetric=wasserstein_distance, aPlot=True, aNormalize=False)\n",
    "\n",
    "heatmaps = getPersitanceHeatMap(aArr=[myPersistenceT], aMax=0.8, aMaxDim=2, aPlot=True)\n",
    "heatmaps = getPersitanceHeatMap(aArr=[myPersistenceW], aMax=2, aMaxDim=2, aPlot=True)\n",
    "heatmaps = getPersitanceHeatMap(aArr=[myPersistenceE], aMax=80, aMaxDim=2, aPlot=True)\n",
    "heatmaps = getPersitanceHeatMap(aArr=[myPersistenceWM], aMax=10, aMaxDim=2, aPlot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPersistenceT = computerPersistenceDiagrams(aMatrix=myMelSpectrogramsCremad[0], aMaxLength=0.8, aMaxDim=2, aMetric=timeBasedEuclideanMetric, aPlot=True, aNormalize=True)\n",
    "myPersistenceW = computerPersistenceDiagrams(aMatrix=myMelSpectrogramsSavee[0], aMaxLength=2, aMaxDim=2, aMetric=wasserstein_distance, aPlot=True, aNormalize=False)\n",
    "myPersistenceE = computerPersistenceDiagrams(aMatrix=myMelSpectrogramsTess[0], aMaxLength=80, aMaxDim=2, aMetric=euclideanMetric, aPlot=True, aNormalize=False)\n",
    "myPersistenceWM = computerPersistenceDiagrams(aMatrix=myMfccRadvess[0], aMaxLength=10, aMaxDim=2, aMetric=wasserstein_distance, aPlot=True, aNormalize=False)\n",
    "\n",
    "heatmaps = getPersitanceHeatMap(aArr=[myPersistenceT], aMax=0.8, aMaxDim=2, aPlot=True)\n",
    "heatmaps = getPersitanceHeatMap(aArr=[myPersistenceW], aMax=2, aMaxDim=2, aPlot=True)\n",
    "heatmaps = getPersitanceHeatMap(aArr=[myPersistenceE], aMax=80, aMaxDim=2, aPlot=True)\n",
    "heatmaps = getPersitanceHeatMap(aArr=[myPersistenceWM], aMax=10, aMaxDim=2, aPlot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelComputePersitence(args):\n",
    "    return computePersistanceFromDb(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResults(aPattern, aResults, aMaxDim, dataset, actors, emotions):\n",
    "    for i in range(len(aResults)):\n",
    "        for k in range(len(aResults[i])):\n",
    "            myDataset = dataset[i]\n",
    "            myActor = actors[i][k]\n",
    "            myEmotion = emotions[i][k]\n",
    "            for j in range(aMaxDim):\n",
    "                np.save(f'./savefiles/{aPattern}_{myDataset}_{myActor}_{myEmotion}_{i}_{k}_{j}.npy', aResults[i][k][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeHeatMaps(aResults, aMaxDim, aMaxDist, aName, dataset, actors, emotions):\n",
    "    myNewHeat = []\n",
    "    for i in range(len(aResults)):\n",
    "        # print(f'Computing {i}')\n",
    "        myHeat = getPersitanceHeatMap(aResults[i], aMax=aMaxDist, aMaxDim=aMaxDim)\n",
    "        myNewHeat.append(myHeat)\n",
    "        for j in range(len(myNewHeat[i])):\n",
    "            # print(j)\n",
    "            myDataset = dataset[i]\n",
    "            myActor = actors[i][j // 2]\n",
    "            myEmotion = emotions[i][j // 2]\n",
    "            np.save(f'./savefiles/{aName}Heat_{myDataset}_{myActor}_{myEmotion}_{i}_{j}.npy', myNewHeat[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdim = 2\n",
    "metrics = [timeBasedEuclideanMetric, wasserstein_distance, euclideanMetric]\n",
    "scaled = [True, False, False]\n",
    "maxdists = [0.8, 2, 80]\n",
    "names = ['timeMetric', 'wasserstein', 'euclidean']\n",
    "for metric, scale, maxdist, name in zip(metrics, scaled, maxdists, names):\n",
    "    args = [\n",
    "        (myMelSpectrogramsCremad, maxdist, maxdim, metric, False, len(myMelSpectrogramsCremad), scale),\n",
    "        (myMelSpectrogramsSavee, maxdist, maxdim, metric, False, len(myMelSpectrogramsSavee), scale),\n",
    "        (myMelSpectrogramsRadvess, maxdist, maxdim, metric, False, len(myMelSpectrogramsRadvess), scale),\n",
    "        (myMelSpectrogramsTess, maxdist, maxdim, metric, False, len(myMelSpectrogramsTess), scale),\n",
    "    ]\n",
    "    results = []\n",
    "    for arg in args:\n",
    "        results.append(parallelComputePersitence(arg))\n",
    "    saveResults(aPattern=name, aResults=results, aMaxDim=maxdim, dataset=['cremad', 'savee', 'radvess', 'tess'], actors=[myCremadActors, mySaveeActors, myRadvessActors, myTessActors], emotions=[myCremadEmotions, mySaveeEmotions, myRadvessEmotions, myTessEmotions])\n",
    "    computeHeatMaps(aResults=results, aMaxDim=maxdim, aMaxDist=maxdist, aName=name, dataset=['cremad', 'savee', 'radvess', 'tess'], actors=[myCremadActors, mySaveeActors, myRadvessActors, myTessActors], emotions=[myCremadEmotions, mySaveeEmotions, myRadvessEmotions, myTessEmotions])\n",
    "    print('To the next!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxDim = 2\n",
    "maxdist = 15\n",
    "args = [\n",
    "    (myMfccCremad, maxdist, maxdim, metric, False, len(myMfccCremad), scale),\n",
    "    (myMfccSavee, maxdist, maxdim, metric, False, len(myMfccSavee), scale),\n",
    "    (myMfccRadvess, maxdist, maxdim, metric, False, len(myMfccRadvess), scale),\n",
    "    (myMfccTess, maxdist, maxdim, metric, False, len(myMfccTess), scale),\n",
    "]\n",
    "results = []\n",
    "for arg in args:\n",
    "    results.append(parallelComputePersitence(arg))\n",
    "saveResults(aPattern='wassersteinMfcc', aResults=results, aMaxDim=maxDim, dataset=['cremad', 'savee', 'radvess', 'tess'], actors=[myCremadActors, mySaveeActors, myRadvessActors, myTessActors], emotions=[myCremadEmotions, mySaveeEmotions, myRadvessEmotions, myTessEmotions])\n",
    "computeHeatMaps(aResults=results, aMaxDim=maxDim, aMaxDist=maxdist, aName='wassersteinMfcc', dataset=['cremad', 'savee', 'radvess', 'tess'], actors=[myCremadActors, mySaveeActors, myRadvessActors, myTessActors], emotions=[myCremadEmotions, mySaveeEmotions, myRadvessEmotions, myTessEmotions])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myMelSpectrogramsCremad = doMelSpectrogram(myCremadSounds, True)\n",
    "myMelSpectrogramsRadvess = doMelSpectrogram(myRadvessSounds, True)\n",
    "myMelSpectrogramsSavee = doMelSpectrogram(mySaveeSounds, True)\n",
    "myMelSpectrogramsTess = doMelSpectrogram(myTessSounds, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(myMelSpectrogramsCremad)):\n",
    "    np.save(f'./savefiles/cremad_{myCremadActors[i]}_{myCremadEmotions[i]}_{i}.npy', myMelSpectrogramsCremad[i])\n",
    "for i in range(len(myMelSpectrogramsRadvess)):\n",
    "    np.save(f'./savefiles/radvess_{myRadvessActors[i]}_{myRadvessEmotions[i]}_{i}.npy', myMelSpectrogramsRadvess[i])\n",
    "for i in range(len(myMelSpectrogramsSavee)):\n",
    "    np.save(f'./savefiles/savee_{mySaveeActors[i]}_{mySaveeEmotions[i]}_{i}.npy', myMelSpectrogramsSavee[i])\n",
    "for i in range(len(myMelSpectrogramsTess)):\n",
    "    np.save(f'./savefiles/tess_{myTessActors[i]}_{myTessEmotions[i]}_{i}.npy', myMelSpectrogramsTess[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './savefiles'\n",
    "\n",
    "def findFilesFromPattern(pattern):\n",
    "    pattern = re.compile(pattern + r'_(\\d+)_(\\d+)_(\\d+)\\.npy')\n",
    "    heatmaps_dict = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            i, j, k = map(int, match.groups())\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            data = np.load(filepath)\n",
    "            \n",
    "            if i not in heatmaps_dict:\n",
    "                heatmaps_dict[i] = []\n",
    "            \n",
    "            while len(heatmaps_dict[i]) <= j:\n",
    "                heatmaps_dict[i].append([])\n",
    "            \n",
    "            while len(heatmaps_dict[i][j]) <= k:\n",
    "                heatmaps_dict[i][j].append(None)\n",
    "            \n",
    "            heatmaps_dict[i][j][k] = data\n",
    "    return [heatmaps_dict[i] for i in sorted(heatmaps_dict.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to your folder\n",
    "folder = \"savefiles/\"\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "    if \"_Actor_\" in filename:\n",
    "        new_name = filename.replace(\"_Actor_\", \"_\")\n",
    "        old_path = os.path.join(folder, filename)\n",
    "        new_path = os.path.join(folder, new_name)\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"Renamed: {filename} → {new_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
