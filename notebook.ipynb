{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import kagglehub\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# import tensorflow_addons as tfa\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from sklearn.datasets import load_iris\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, BatchNormalization, Dropout, Conv2D, MaxPooling2D, Flatten, AveragePooling2D, Activation, Concatenate\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy, TopKCategoricalAccuracy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n",
    "import logging, os\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "from scipy.io import wavfile\n",
    "from scipy.fftpack import fft\n",
    "import cv2\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "from glob import glob\n",
    "import skimage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint,LearningRateScheduler\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Subset\n",
    "from torchmetrics.classification import MulticlassAUROC, MulticlassAccuracy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.metrics import Metric\n",
    "\n",
    "if os.path.isfile(\"myData.npz\"):\n",
    "# if False:\n",
    "    print(\"✅ 'data.npz' exists.\")\n",
    "    with np.load(\"myData.npz\") as data:\n",
    "        myData = data['myData']\n",
    "        myData2 = data['myData2']\n",
    "        myData3 = data['myData3']\n",
    "        myY = data['myY']\n",
    "        myActors = data['myActors']\n",
    "        myDatasets = data['myDatasets']\n",
    "        myHasNoise = data['myHasNoise']\n",
    "        print(np.unique(myY))\n",
    "        print(np.unique(myActors))\n",
    "else:\n",
    "\n",
    "    folder = './savefiles'\n",
    "\n",
    "    def findFilesFromPattern(pattern):\n",
    "        pattern = re.compile(pattern + r'_(.*?)_(.*?)_(.*?)_(\\d+)_(\\d+)\\.npy')\n",
    "        heatmaps_dict = {}\n",
    "\n",
    "        for filename in os.listdir(folder):\n",
    "            match = pattern.match(filename)\n",
    "            if match:\n",
    "                dataset, actor, emotion, i, j = map(str, match.groups())\n",
    "                i, j = int(i), int(j)\n",
    "                filepath = os.path.join(folder, filename)\n",
    "                data = np.load(filepath)\n",
    "\n",
    "                heatmaps_dict[f'{dataset}_{actor}_{emotion}_{j // 2}_{j%2}'] = {'data': data, 'dataset': dataset, 'actor': actor, 'emotion':emotion, 'type': j, 'has_noise': (j%4 == 0 or j%4==1)}\n",
    "\n",
    "        return heatmaps_dict\n",
    "\n",
    "    mfccwasserstein = findFilesFromPattern('wassersteinMfccHeat')\n",
    "    melwasserstein = findFilesFromPattern('wassersteinHeat')\n",
    "    meltimeeuclid = findFilesFromPattern('timeMetricHeat')\n",
    "    meleuclid = findFilesFromPattern('euclideanHeat')\n",
    "\n",
    "    def load_spectrograms(prefixes, path='./savefiles'):\n",
    "        patterns = []\n",
    "        for prefix in prefixes:\n",
    "            patterns.append(os.path.join(path, f\"{prefix}_*.npy\"))\n",
    "        my_globs = glob(patterns[0])\n",
    "        for pattern in patterns[1:]:\n",
    "            my_globs = my_globs + glob(pattern)\n",
    "        file_list = sorted(my_globs)\n",
    "        return [np.load(file) for i, file in enumerate(file_list)]\n",
    "\n",
    "    myRaw = load_spectrograms([\"savee\", 'tess', 'radvess', 'cremad'])\n",
    "    print(len(mfccwasserstein))\n",
    "    print(len([mfccwasserstein[key]['data'] for key in sorted(mfccwasserstein.keys()) if mfccwasserstein[key]['type'] % 2 == 0]))\n",
    "    print(len([mfccwasserstein[key]['data'] for key in sorted(mfccwasserstein.keys()) if mfccwasserstein[key]['type'] % 2 == 1]))\n",
    "    print(np.array([[meleuclid[key]['data'] for key in sorted(meleuclid.keys()) if meleuclid[key]['type'] == 0]]).shape)\n",
    "\n",
    "    myRaw2 = load_spectrograms([\"mfcc_savee\", 'mfcc_tess', 'mfcc_radvess', 'mfcc_cremad'])\n",
    "\n",
    "    print(len(myRaw2))\n",
    "    print(len(myRaw2[0]))\n",
    "\n",
    "    myData = np.array([myRaw])\n",
    "    myData3 = np.stack([myRaw2])\n",
    "    print('finish data')\n",
    "    myData = myData.astype('float16')\n",
    "    myData = np.transpose(myData, (1, 2, 3, 0))\n",
    "\n",
    "    myData3 = myData3.astype('float16')\n",
    "    myData3 = np.transpose(myData3, (1, 2, 3, 0))\n",
    "    myEmotionMap = {\n",
    "        'neutral': 1, 'calm':2, 'happy':3, 'sad':4, 'angry':5, 'fearful':6, 'disgust':7, 'surprised':8\n",
    "    }\n",
    "    myY = np.array(\n",
    "        [myEmotionMap[mfccwasserstein[key]['emotion']] -1 for key in sorted(mfccwasserstein.keys()) if mfccwasserstein[key]['type'] % 2 == 0]\n",
    "    )\n",
    "    print(np.unique(myY))\n",
    "    myActors = np.array(\n",
    "        [mfccwasserstein[key]['actor'] + '_' + mfccwasserstein[key]['dataset']  for key in sorted(mfccwasserstein.keys()) if mfccwasserstein[key]['type'] % 2 == 0]\n",
    "    )\n",
    "    myDatasets = np.array(\n",
    "        [mfccwasserstein[key]['dataset']  for key in sorted(mfccwasserstein.keys()) if mfccwasserstein[key]['type'] % 2 == 0]\n",
    "    )\n",
    "    myHasNoise = np.array(\n",
    "        [mfccwasserstein[key]['has_noise']  for key in sorted(mfccwasserstein.keys()) if mfccwasserstein[key]['type'] % 2 == 0]\n",
    "    )\n",
    "    print(np.unique(myActors))\n",
    "\n",
    "    print(np.unique(myY))\n",
    "    myY = [x -1 if x > 0 else x for x in myY]\n",
    "    myY = to_categorical(myY, num_classes=6)\n",
    "\n",
    "    myData2 = np.array([\n",
    "                        [meleuclid[key]['data'] for key in sorted(meleuclid.keys()) if meleuclid[key]['type'] % 2 == 0],\n",
    "                        [meleuclid[key]['data'] for key in sorted(meleuclid.keys()) if meleuclid[key]['type'] % 2 == 1],\n",
    "                        [meltimeeuclid[key]['data'] for key in sorted(meltimeeuclid.keys()) if meltimeeuclid[key]['type'] % 2 == 0],\n",
    "                        [meltimeeuclid[key]['data'] for key in sorted(meltimeeuclid.keys()) if meltimeeuclid[key]['type'] % 2 == 1],\n",
    "                        # [mfccwasserstein[key]['data'] for key in sorted(mfccwasserstein.keys()) if mfccwasserstein[key]['type'] % 2 == 0],\n",
    "                        # [mfccwasserstein[key]['data'] for key in sorted(mfccwasserstein.keys()) if mfccwasserstein[key]['type'] % 2 == 1],\n",
    "                        [melwasserstein[key]['data'] for key in sorted(melwasserstein.keys()) if melwasserstein[key]['type'] % 2 == 0],\n",
    "                        [melwasserstein[key]['data'] for key in sorted(melwasserstein.keys()) if melwasserstein[key]['type'] % 2 == 1]\n",
    "                        ])\n",
    "    print('finish data')\n",
    "    myData2 = myData2.astype('float16')\n",
    "    print(myData2.shape)\n",
    "    myData2 = np.transpose(myData2, (1, 2, 3, 0))\n",
    "    print(myData2.shape)\n",
    "\n",
    "    def upload_to_gcs(bucket_name, source_file, destination_blob):\n",
    "        \"\"\"Upload a file to a GCS bucket.\"\"\"\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob)\n",
    "        blob.upload_from_filename(source_file)\n",
    "        print(f\"✅ Uploaded {destination_blob} to gs://{bucket_name}/{destination_blob}\")\n",
    "\n",
    "    np.savez_compressed(\n",
    "        \"myData.npz\",\n",
    "        myData=myData,\n",
    "        myData2=myData2,\n",
    "        myData3=myData3,\n",
    "        myY=myY,\n",
    "        myActors=myActors,\n",
    "        myDatasets=myDatasets,\n",
    "        myHasNoise=myHasNoise\n",
    "    )\n",
    "\n",
    "    GCS_BUCKET = \"simplicialcomplex-outputbucket\"\n",
    "\n",
    "    # upload_to_gcs(GCS_BUCKET, \"myData.npz\", \"data/myData.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.unique([str(x.shape) for x in myRaw2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(test_size=0.2, n_splits=1)\n",
    "groups = myActors#np.array([f\"{d}_{a}_{c}\" for d, a, c in zip(myActors, myDatasets)])\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def stratified_group_split_3way(y, z, groups, val_size=0.2, test_size=0.2, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    df = pd.DataFrame({'y': y, 'z': z, 'group': groups})\n",
    "\n",
    "    df[\"strat_label\"] = df[\"y\"].astype(str) + \"_\" + df[\"z\"].astype(str)\n",
    "\n",
    "    group_labels = (\n",
    "        df.groupby('group')['y']\n",
    "          .agg(lambda s: s.value_counts().index[0])\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    sss_outer = StratifiedShuffleSplit(\n",
    "        n_splits=1, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    sss_inner = StratifiedShuffleSplit(\n",
    "        n_splits=1, test_size=val_size/(1 - test_size), random_state=random_state + 1\n",
    "    )\n",
    "\n",
    "    group_indices = np.arange(len(group_labels))\n",
    "    for trainval_g, test_g in sss_outer.split(group_indices, group_labels['y']):\n",
    "        trainval_groups = group_labels['group'].iloc[trainval_g].values\n",
    "        test_groups = group_labels['group'].iloc[test_g].values\n",
    "\n",
    "        trainval_df = group_labels.iloc[trainval_g]\n",
    "        trainval_idx = np.arange(len(trainval_df))\n",
    "\n",
    "        for train_g, val_g in sss_inner.split(trainval_idx, trainval_df['y']):\n",
    "            train_groups = trainval_df['group'].iloc[train_g].values\n",
    "            val_groups = trainval_df['group'].iloc[val_g].values\n",
    "\n",
    "    # Map back to samples\n",
    "    train_mask = df['group'].isin(train_groups)\n",
    "    val_mask = df['group'].isin(val_groups)\n",
    "    test_mask = df['group'].isin(test_groups)\n",
    "\n",
    "    train_idx = np.where(train_mask)[0]\n",
    "    val_idx = np.where(val_mask)[0]\n",
    "    test_idx = np.where(test_mask)[0]\n",
    "\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "def stratified_group_split_cover_z_3way(\n",
    "    y,\n",
    "    z,\n",
    "    groups,\n",
    "    val_size=0.2,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    keep_size=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    3-way group-wise split (train/val/test) with:\n",
    "      - stratification by composite (y,z)\n",
    "      - at least one of each z in test\n",
    "      - no group overlap between splits\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array-like\n",
    "        Primary labels (e.g. emotion IDs)\n",
    "    z : array-like\n",
    "        Secondary variable (e.g. gender, dataset)\n",
    "    groups : array-like\n",
    "        Group IDs (e.g. actors)\n",
    "    val_size : float\n",
    "        Fraction of remaining groups for validation (after test split)\n",
    "    test_size : float\n",
    "        Fraction of all groups for test\n",
    "    random_state : int\n",
    "        RNG seed\n",
    "    keep_size : bool\n",
    "        Try to maintain split sizes even when adding missing z-groups\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    df = pd.DataFrame({\"y\": y, \"z\": z, \"group\": groups})\n",
    "\n",
    "    # Composite stratification label\n",
    "    df[\"strat\"] = df[\"y\"].astype(str) + \"_\" + df[\"z\"].astype(str)\n",
    "\n",
    "    # Majority composite per group\n",
    "    grp = (\n",
    "        df.groupby(\"group\")[\"y\"]\n",
    "          .agg(lambda s: s.value_counts().index[0])\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # --- Step 1: TrainVal/Test Split ---\n",
    "    sss_outer = StratifiedShuffleSplit(\n",
    "        n_splits=1, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    g_idx = np.arange(len(grp))\n",
    "    for trainval_g_idx, test_g_idx in sss_outer.split(g_idx, grp[\"y\"]):\n",
    "        trainval_groups = set(grp[\"group\"].iloc[trainval_g_idx])\n",
    "        test_groups = set(grp[\"group\"].iloc[test_g_idx])\n",
    "\n",
    "    # --- Ensure all z categories appear in test ---\n",
    "    all_z = set(df[\"z\"].unique())\n",
    "    def z_in_groups(gset):\n",
    "        return set(df[df[\"group\"].isin(gset)][\"z\"].unique())\n",
    "\n",
    "    test_z = z_in_groups(test_groups)\n",
    "    missing_z = list(all_z - test_z)\n",
    "\n",
    "    if missing_z:\n",
    "        g2count = df.groupby(\"group\").size().to_dict()\n",
    "        z_to_groups = {zv: set(df.loc[df[\"z\"] == zv, \"group\"].unique()) for zv in all_z}\n",
    "\n",
    "        for zv in missing_z:\n",
    "            candidates = z_to_groups[zv] & trainval_groups\n",
    "            if not candidates:\n",
    "                continue\n",
    "            g_chosen = min(candidates, key=lambda g: g2count.get(g, 0))\n",
    "            trainval_groups.remove(g_chosen)\n",
    "            test_groups.add(g_chosen)\n",
    "\n",
    "            if keep_size:\n",
    "                removable = []\n",
    "                test_z_after = z_in_groups(test_groups)\n",
    "                for g in list(test_groups - {g_chosen}):\n",
    "                    tmp = test_groups.copy()\n",
    "                    tmp.remove(g)\n",
    "                    if z_in_groups(tmp) == test_z_after:\n",
    "                        removable.append(g)\n",
    "                if removable:\n",
    "                    g_back = max(removable, key=lambda g: g2count.get(g, 0))\n",
    "                    test_groups.remove(g_back)\n",
    "                    trainval_groups.add(g_back)\n",
    "\n",
    "    # --- Step 2: Train/Val Split (from remaining groups) ---\n",
    "    subgrp = (\n",
    "        df[df[\"group\"].isin(trainval_groups)]\n",
    "        .groupby(\"group\")[\"y\"]\n",
    "        .agg(lambda s: s.value_counts().index[0])\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    sss_inner = StratifiedShuffleSplit(\n",
    "        n_splits=1,\n",
    "        test_size=val_size / (1 - test_size),\n",
    "        random_state=random_state + 1,\n",
    "    )\n",
    "    g_idx_inner = np.arange(len(subgrp))\n",
    "    for train_g_idx, val_g_idx in sss_inner.split(g_idx_inner, subgrp[\"y\"]):\n",
    "        train_groups = set(subgrp[\"group\"].iloc[train_g_idx])\n",
    "        val_groups = set(subgrp[\"group\"].iloc[val_g_idx])\n",
    "\n",
    "    # --- Map groups to sample indices ---\n",
    "    train_idx = np.where(df[\"group\"].isin(train_groups))[0]\n",
    "    val_idx   = np.where(df[\"group\"].isin(val_groups))[0]\n",
    "    test_idx  = np.where(df[\"group\"].isin(test_groups))[0]\n",
    "\n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_idx, val_idx, test_idx = stratified_group_split_cover_z_3way(y=np.argmax(myY, axis=1), z=myDatasets, groups=groups, val_size=0.2, test_size=0.2)\n",
    "\n",
    "X_train, X_val, X_test, X_train2, X_val2, X_test2, X_train3, X_val3, X_test3 = myData[train_idx], myData[val_idx], myData[test_idx], myData2[train_idx], myData2[val_idx], myData2[test_idx], myData3[train_idx], myData3[val_idx], myData3[test_idx]\n",
    "y_train, y_val, y_test = myY[train_idx], myY[val_idx], myY[test_idx]\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "has_noise_idx = np.where(myHasNoise[train_idx] == 0)[0]\n",
    "X_train_tensor = torch.tensor(X_train[has_noise_idx].transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "X_train2_tensor = torch.tensor(X_train2[has_noise_idx].transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "X_train3_tensor = torch.tensor(X_train3[has_noise_idx].transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(np.argmax(y_train[has_noise_idx], axis=1), dtype=torch.long)\n",
    "\n",
    "has_noise_idx = np.where(myHasNoise[val_idx] == 0)[0]\n",
    "X_val_tensor = torch.tensor(X_val[has_noise_idx].transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "X_val2_tensor = torch.tensor(X_val2[has_noise_idx].transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "X_val3_tensor = torch.tensor(X_val3[has_noise_idx].transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(np.argmax(y_val[has_noise_idx], axis=1), dtype=torch.long)\n",
    "\n",
    "has_noise_idx = np.where(myHasNoise[test_idx] == 0)[0]\n",
    "X_test_tensor = torch.tensor(X_test[has_noise_idx].transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "X_test2_tensor = torch.tensor(X_test2[has_noise_idx].transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "X_test3_tensor = torch.tensor(X_test3[has_noise_idx].transpose(0, 3, 1, 2), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(np.argmax(y_test[has_noise_idx], axis=1), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 64),  # for input 32×32 after two poolings\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class CNNModel2(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(CNNModel2, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(6, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = CNNModel(64)\n",
    "        self.model2 = CNNModel2(64)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.model(x1)\n",
    "        x2 = self.model2(x2)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple GPU\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU (if any)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # fallback\n",
    "\n",
    "auroc = MulticlassAUROC(num_classes=6, average='macro').to(device)\n",
    "top3acc = MulticlassAccuracy(num_classes=6, top_k=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(myActors[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(myActors[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(myActors[val_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Instantiate model ---\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "dataset = TensorDataset(X_train_tensor, X_train2_tensor, y_train_tensor)\n",
    "val_ds = TensorDataset(X_val_tensor, X_val2_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=256, shuffle=True,\n",
    "                          num_workers=4,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=256,num_workers=4,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, X_test2_tensor, y_test_tensor), batch_size=256)\n",
    "\n",
    "\n",
    "\n",
    "# model = CNNModel().to(device)\n",
    "# model2 = CNNModel2().to(device)\n",
    "\n",
    "\n",
    "# fusion = FusionNetGated().to(device)\n",
    "\n",
    "model_comb = CombinedModel(num_classes=6).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# optimizer2 = torch.optim.Adam(model2.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# fusion_optimizer = torch.optim.Adam(fusion.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "optimizer_combined = torch.optim.Adam(model_comb.parameters())\n",
    "num_epochs = 30\n",
    "\n",
    "best_val_auc = 0\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# Training Loop with Checkpoint\n",
    "# ================================================================\n",
    "for epoch in range(num_epochs):\n",
    "    model_comb.train()\n",
    "    train_preds, train_labels = [], []\n",
    "    for X_batch, X2_batch, y_batch in train_loader:\n",
    "        X_batch, X2_batch, y_batch = X_batch.to(device), X2_batch.to(device), y_batch.to(device)\n",
    "        # optimizer.zero_grad()\n",
    "        # optimizer2.zero_grad()\n",
    "        # fusion_optimizer.zero_grad()\n",
    "        optimizer_combined.zero_grad()\n",
    "\n",
    "        # outputs = model(X_batch)\n",
    "        # outputs2 = model2(X2_batch)\n",
    "\n",
    "        # combined_logits, alpha = fusion(outputs, outputs2)\n",
    "        combined_logits = model_comb(X_batch, X2_batch)\n",
    "        loss = criterion(combined_logits, y_batch)\n",
    "\n",
    "        # optional deep supervision helps\n",
    "        # loss = loss + 0.2*criterion(outputs, y_batch) + 0.2*criterion(outputs2, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        train_preds.append(torch.softmax(combined_logits, dim=1))\n",
    "        train_labels.append(y_batch)\n",
    "\n",
    "        # optimizer.step()\n",
    "\n",
    "        # optimizer2.step()\n",
    "\n",
    "        # fusion_optimizer.step()\n",
    "\n",
    "        optimizer_combined.step()\n",
    "\n",
    "    # Validation\n",
    "    model_comb.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_val, X2_val, y_val in val_loader:\n",
    "            X_val, X2_val, y_val = X_val.to(device), X2_val.to(device), y_val.to(device)\n",
    "            # outputs = model(X_val)\n",
    "            # outputs2 = model2(X2_val)\n",
    "\n",
    "            # combined_logits = fusion(outputs, outputs2)\n",
    "            # combined_logits, alpha = fusion(outputs, outputs2)\n",
    "\n",
    "            combined_logits = model_comb(X_val, X2_val)\n",
    "            # loss = criterion(combined_logits, y_val)\n",
    "\n",
    "            val_loss = criterion(combined_logits, y_val)\n",
    "            preds = torch.softmax(combined_logits, dim=1)\n",
    "            val_preds.append(preds)\n",
    "            val_labels.append(y_val)\n",
    "    val_preds = torch.cat(val_preds)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "\n",
    "    val_auc = auroc(val_preds, val_labels).item()\n",
    "    val_top3 = top3acc(val_preds, val_labels).item()\n",
    "\n",
    "    y_pred = torch.argmax(val_preds, dim=1)\n",
    "\n",
    "    accuracy = (y_pred == val_labels).float().mean()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - val_auc: {val_auc:.4f} - top3_acc: {val_top3:.4f} - val_acc: {accuracy.item():.4f} - val_loss {val_loss.item():.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        # torch.save(model.state_dict(), \"best_model_full.pth\")\n",
    "        # torch.save(model2.state_dict(), \"best_model_full2.pth\")\n",
    "        # torch.save(fusion.state_dict(), \"best_model_full3.pth\")\n",
    "        torch.save(model_comb.state_dict(), \"best_model_comb.pth\")\n",
    "        print(\"✅ Saved new best model.\")\n",
    "\n",
    "    train_preds = torch.cat(train_preds)\n",
    "    train_labels = torch.cat(train_labels)\n",
    "    val_auc = auroc(train_preds, train_labels).item()\n",
    "    val_top3 = top3acc(train_preds, train_labels).item()\n",
    "\n",
    "    y_pred = torch.argmax(train_preds, dim=1)\n",
    "\n",
    "    accuracy = (y_pred == train_labels).float().mean()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - train_auc: {val_auc:.4f} - train_top3_acc: {val_top3:.4f} - train_acc: {accuracy.item():.4f} - train_loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# Evaluation\n",
    "# ================================================================\n",
    "# model.load_state_dict(torch.load(\"best_model_full.pth\"))\n",
    "# model2.load_state_dict(torch.load(\"best_model_full2.pth\"))\n",
    "# fusion.load_state_dict(torch.load(\"best_model_full3.pth\"))\n",
    "# model.eval()\n",
    "# model2.eval()\n",
    "# fusion.eval()\n",
    "model_comb.load_state_dict(torch.load(\"best_model_comb.pth\"))\n",
    "model_comb.eval()\n",
    "\n",
    "all_preds, all_labels, all_preds2, all_labels2 = [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, X2_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        X2_batch = X2_batch.to(device)\n",
    "        # outputs = model(X_batch)\n",
    "        # outputs2 = model2(X2_batch)\n",
    "\n",
    "        # combined_logits = fusion(outputs, outputs2)\n",
    "        # combined_logits = fusion(outputs, outputs2)\n",
    "        combined_logits = model_comb(X_batch, X2_batch)\n",
    "        # preds = torch.softmax(combined_logits, dim=1)\n",
    "        preds = torch.argmax(combined_logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_preds2.append(torch.softmax(combined_logits, dim=1))\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "        all_labels2.append(y_batch.to(device))\n",
    "\n",
    "    val_preds = torch.cat(all_preds2)\n",
    "    val_labels = torch.cat(all_labels2)\n",
    "    val_auc = auroc(val_preds, val_labels).item()\n",
    "    val_top3 = top3acc(val_preds, val_labels).item()\n",
    "\n",
    "    y_pred = torch.argmax(val_preds, dim=1)\n",
    "\n",
    "    accuracy = (y_pred == val_labels).float().mean()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - val_auc: {val_auc:.4f} - top3_acc: {val_top3:.4f} - val_acc: {accuracy.item():.4f}\")\n",
    "\n",
    "# Classification report\n",
    "class_labels = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust']\n",
    "report = classification_report(all_labels, all_preds, target_names=class_labels)\n",
    "print(report)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_df = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 64),  # for input 32×32 after two poolings\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),  # for input 32×32 after two poolings\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_ds = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=256)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=256)\n",
    "\n",
    "model = CNNModel().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "num_epochs = 20\n",
    "\n",
    "best_val_auc = 0.0\n",
    "\n",
    "# ================================================================\n",
    "# Training Loop with Checkpoint\n",
    "# ================================================================\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs = model(X_val)\n",
    "            preds = torch.softmax(outputs, dim=1)\n",
    "            val_preds.append(preds)\n",
    "            val_labels.append(y_val)\n",
    "    val_preds = torch.cat(val_preds)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "\n",
    "    val_auc = auroc(val_preds, val_labels).item()\n",
    "    val_top3 = top3acc(val_preds, val_labels).item()\n",
    "\n",
    "    y_pred = torch.argmax(val_preds, dim=1)\n",
    "\n",
    "    accuracy = (y_pred == val_labels).float().mean()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - val_auc: {val_auc:.4f} - top3_acc: {val_top3:.4f} - val_acc: {accuracy.item():.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), \"best_model_mel.pth\")\n",
    "        print(\"✅ Saved new best model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# Evaluation\n",
    "# ================================================================\n",
    "model.load_state_dict(torch.load(\"best_model_mel.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels, all_preds2, all_labels2 = [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_preds2.append(torch.softmax(outputs, dim=1))\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "        all_labels2.append(y_batch.to(device))\n",
    "\n",
    "    val_preds = torch.cat(all_preds2)\n",
    "    val_labels = torch.cat(all_labels2)\n",
    "    val_auc = auroc(val_preds, val_labels).item()\n",
    "    val_top3 = top3acc(val_preds, val_labels).item()\n",
    "\n",
    "    y_pred = torch.argmax(val_preds, dim=1)\n",
    "\n",
    "    accuracy = (y_pred == val_labels).float().mean()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - val_auc: {val_auc:.4f} - top3_acc: {val_top3:.4f} - val_acc: {accuracy.item():.4f}\")\n",
    "\n",
    "\n",
    "# Classification report\n",
    "class_labels = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust']\n",
    "report = classification_report(all_labels, all_preds, target_names=class_labels)\n",
    "print(report)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_df = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel2(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(CNNModel2, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(6, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = TensorDataset(X_train2_tensor, y_train_tensor)\n",
    "val_ds = TensorDataset(X_val2_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=256)\n",
    "test_loader = DataLoader(TensorDataset(X_test2_tensor, y_test_tensor), batch_size=256)\n",
    "\n",
    "model = CNNModel2().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "num_epochs = 40\n",
    "\n",
    "best_val_auc = 0.0\n",
    "\n",
    "# ================================================================\n",
    "# Training Loop with Checkpoint\n",
    "# ================================================================\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs = model(X_val)\n",
    "            preds = torch.softmax(outputs, dim=1)\n",
    "            val_preds.append(preds)\n",
    "            val_labels.append(y_val)\n",
    "    val_preds = torch.cat(val_preds)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "\n",
    "    val_auc = auroc(val_preds, val_labels).item()\n",
    "    val_top3 = top3acc(val_preds, val_labels).item()\n",
    "\n",
    "    y_pred = torch.argmax(val_preds, dim=1)\n",
    "\n",
    "    accuracy = (y_pred == val_labels).float().mean()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - val_auc: {val_auc:.4f} - top3_acc: {val_top3:.4f} - val_acc: {accuracy.item():.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), \"best_model_per.pth\")\n",
    "        print(\"✅ Saved new best model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# Evaluation\n",
    "# ================================================================\n",
    "model.load_state_dict(torch.load(\"best_model_per.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels, all_preds2, all_labels2 = [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_preds2.append(torch.softmax(outputs, dim=1))\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "        all_labels2.append(y_batch.to(device))\n",
    "\n",
    "    val_preds = torch.cat(all_preds2)\n",
    "    val_labels = torch.cat(all_labels2)\n",
    "    val_auc = auroc(val_preds, val_labels).item()\n",
    "    val_top3 = top3acc(val_preds, val_labels).item()\n",
    "\n",
    "    y_pred = torch.argmax(val_preds, dim=1)\n",
    "\n",
    "    accuracy = (y_pred == val_labels).float().mean()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - val_auc: {val_auc:.4f} - top3_acc: {val_top3:.4f} - val_acc: {accuracy.item():.4f}\")\n",
    "\n",
    "\n",
    "# Classification report\n",
    "class_labels = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust']\n",
    "report = classification_report(all_labels, all_preds, target_names=class_labels)\n",
    "print(report)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_df = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel3(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(CNNModel3, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.AvgPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = TensorDataset(X_train3_tensor, y_train_tensor)\n",
    "val_ds = TensorDataset(X_val3_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=256)\n",
    "test_loader = DataLoader(TensorDataset(X_test3_tensor, y_test_tensor), batch_size=256)\n",
    "\n",
    "model = CNNModel3().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "num_epochs = 50\n",
    "\n",
    "best_val_auc = 0.0\n",
    "\n",
    "# ================================================================\n",
    "# Training Loop with Checkpoint\n",
    "# ================================================================\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs = model(X_val)\n",
    "            preds = torch.softmax(outputs, dim=1)\n",
    "            val_preds.append(preds)\n",
    "            val_labels.append(y_val)\n",
    "    val_preds = torch.cat(val_preds)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "\n",
    "    val_auc = auroc(val_preds, val_labels).item()\n",
    "    val_top3 = top3acc(val_preds, val_labels).item()\n",
    "\n",
    "    y_pred = torch.argmax(val_preds, dim=1)\n",
    "\n",
    "    accuracy = (y_pred == val_labels).float().mean()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - val_auc: {val_auc:.4f} - top3_acc: {val_top3:.4f} - val_acc: {accuracy.item():.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), \"best_model_per.pth\")\n",
    "        print(\"✅ Saved new best model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================\n",
    "# Evaluation\n",
    "# ================================================================\n",
    "model.load_state_dict(torch.load(\"best_model_per.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels, all_preds2, all_labels2 = [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_preds2.append(torch.softmax(outputs, dim=1))\n",
    "        all_labels.extend(y_batch.numpy())\n",
    "        all_labels2.append(y_batch.to(device))\n",
    "\n",
    "    val_preds = torch.cat(all_preds2)\n",
    "    val_labels = torch.cat(all_labels2)\n",
    "    val_auc = auroc(val_preds, val_labels).item()\n",
    "    val_top3 = top3acc(val_preds, val_labels).item()\n",
    "\n",
    "    y_pred = torch.argmax(val_preds, dim=1)\n",
    "\n",
    "    accuracy = (y_pred == val_labels).float().mean()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - val_auc: {val_auc:.4f} - top3_acc: {val_top3:.4f} - val_acc: {accuracy.item():.4f}\")\n",
    "\n",
    "\n",
    "# Classification report\n",
    "class_labels = ['neutral', 'happy', 'sad', 'angry', 'fearful', 'disgust']\n",
    "report = classification_report(all_labels, all_preds, target_names=class_labels)\n",
    "print(report)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cm_df = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
